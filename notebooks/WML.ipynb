{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weighted Mutual Learning (WML), an alternative to distillation ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Pruning function : \n",
    "This function does the following:\n",
    "\n",
    "1. Calculates filter importance using the SNIP method.\n",
    "2. Computes layer importance based on average filter importance.\n",
    "3. Normalizes layer importance.\n",
    "4. Determines the number of parameters to prune per layer.\n",
    "5. Prunes filters in each layer based on their importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extended_snip_pruning_gpt2(model, X, Y, pruning_ratio, n_head, layer_bound=0.9):\n",
    "    # Enable gradients for all parameters\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    # Count non-zero parameters before pruning\n",
    "    total_params_before = sum(p.numel() for p in model.parameters())\n",
    "    non_zero_params_before = sum((p != 0).sum().item() for p in model.parameters())\n",
    "    print(f\"Total parameters before pruning: {total_params_before:,}\")\n",
    "    print(f\"Non-zero parameters before pruning: {non_zero_params_before:,}\")\n",
    "\n",
    "    # Forward pass\n",
    "    loss = model(X, Y)\n",
    "\n",
    "    # Backward pass\n",
    "    loss[1].backward()\n",
    "\n",
    "    # Calculate importance for attention heads and feed-forward layers\n",
    "    attention_importance = {}\n",
    "    ffn_importance = {}\n",
    "    \n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            if \"attn.c_attn\" in name:\n",
    "                importance = torch.sum(torch.abs(module.weight.grad * module.weight), dim=1)\n",
    "                attention_importance[name] = importance.view(3, n_head, -1).sum(dim=(0, 2))\n",
    "            elif \"attn.c_proj\" in name:\n",
    "                importance = torch.sum(torch.abs(module.weight.grad * module.weight), dim=0)\n",
    "                attention_importance[name] = importance.view(n_head, -1).sum(dim=1)\n",
    "            elif \"mlp.c_fc\" in name:\n",
    "                importance = torch.sum(torch.abs(module.weight.grad * module.weight), dim=1)\n",
    "                ffn_importance[name] = importance\n",
    "            elif \"mlp.c_proj\" in name:\n",
    "                importance = torch.sum(torch.abs(module.weight.grad * module.weight), dim=0)\n",
    "                ffn_importance[name] = importance\n",
    "\n",
    "    # Calculate layer importance\n",
    "    layer_importance = {}\n",
    "    for name, importance in attention_importance.items():\n",
    "        layer_name = name.rsplit(\".\", 2)[0]\n",
    "        layer_importance[layer_name] = torch.mean(importance)\n",
    "    \n",
    "    for name, importance in ffn_importance.items():\n",
    "        layer_name = name.rsplit(\".\", 2)[0]\n",
    "        layer_importance[layer_name] = (layer_importance.get(layer_name, 0) + torch.mean(importance)) / 2\n",
    "\n",
    "    # Normalize layer importance\n",
    "    total_importance = sum(layer_importance.values())\n",
    "    normalized_importance = {name: imp / total_importance for name, imp in layer_importance.items()}\n",
    "\n",
    "    # Calculate number of elements to prune per layer\n",
    "    total_elements = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    elements_to_prune = int(pruning_ratio * total_elements)\n",
    "    layer_prune_elements = {name: int(imp * elements_to_prune) for name, imp in normalized_importance.items()}\n",
    "\n",
    "    # Prune attention heads and feed-forward neurons\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            layer_name = name.rsplit(\".\", 2)[0]\n",
    "            if \"attn.c_attn\" in name:\n",
    "                head_importance = attention_importance[name]\n",
    "                heads_to_keep = max(1, int((1 - layer_bound) * n_head))\n",
    "                _, indices = torch.topk(head_importance, k=heads_to_keep, largest=True)\n",
    "                mask = torch.zeros(n_head, device=module.weight.device)\n",
    "                mask[indices] = 1\n",
    "                mask = mask.repeat_interleave(module.weight.size(0) // n_head).unsqueeze(1).expand_as(module.weight)\n",
    "                module.weight.data *= mask\n",
    "                if module.bias is not None:\n",
    "                    module.bias.data *= mask.squeeze()\n",
    "            elif \"attn.c_proj\" in name:\n",
    "                head_importance = attention_importance[name]\n",
    "                heads_to_keep = max(1, int((1 - layer_bound) * n_head))\n",
    "                _, indices = torch.topk(head_importance, k=heads_to_keep, largest=True)\n",
    "                mask = torch.zeros(n_head, device=module.weight.device)\n",
    "                mask[indices] = 1\n",
    "                mask = mask.repeat_interleave(module.weight.size(1) // n_head).unsqueeze(0).expand_as(module.weight)\n",
    "                module.weight.data *= mask\n",
    "            elif \"mlp.c_fc\" in name:\n",
    "                neuron_importance = ffn_importance[name]\n",
    "                neurons_to_keep = max(1, int((1 - layer_bound) * module.out_features))\n",
    "                _, indices = torch.topk(neuron_importance, k=neurons_to_keep, largest=True)\n",
    "                mask = torch.zeros(module.out_features, device=module.weight.device)\n",
    "                mask[indices] = 1\n",
    "                module.weight.data *= mask.unsqueeze(1).expand_as(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    module.bias.data *= mask\n",
    "\n",
    "            elif \"mlp.c_proj\" in name:\n",
    "                neuron_importance = ffn_importance[name]\n",
    "                neurons_to_keep = max(1, int((1 - layer_bound) * module.in_features))\n",
    "                _, indices = torch.topk(neuron_importance, k=neurons_to_keep, largest=True)\n",
    "                mask = torch.zeros(module.in_features, device=module.weight.device)\n",
    "                mask[indices] = 1\n",
    "                module.weight.data *= mask.unsqueeze(0).expand_as(module.weight)\n",
    "\n",
    "    # Reset gradients\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Count non-zero parameters after pruning\n",
    "    total_params_after = sum(p.numel() for p in model.parameters())\n",
    "    non_zero_params_after = sum((p != 0).sum().item() for p in model.parameters())\n",
    "    print(f\"Total parameters after pruning: {total_params_after:,}\")\n",
    "    print(f\"Non-zero parameters after pruning: {non_zero_params_after:,}\")\n",
    "    print(f\"Effectively pruned {non_zero_params_before - non_zero_params_after:,} parameters\")\n",
    "    print(f\"Effective pruning ratio: {(non_zero_params_before - non_zero_params_after) / non_zero_params_before:.2%}\")\n",
    "\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def wml_loss(outputs, labels, peer_outputs, weights, alpha):\n",
    "    \"\"\"\n",
    "    Compute the Weighted Mutual Learning loss for GPT-2.\n",
    "    \n",
    "    :param outputs: Logits from the current peer model\n",
    "    :param labels: True labels (input_ids for GPT-2)\n",
    "    :param peer_outputs: List of logits from other peer models\n",
    "    :param weights: Weights for each peer model\n",
    "    :param alpha: Balancing factor between CE loss and KL divergence\n",
    "    \"\"\"\n",
    "    # Cross-entropy loss\n",
    "    ce_loss = F.cross_entropy(outputs.squeeze(1), labels)\n",
    "    \n",
    "    # KL divergence loss\n",
    "    kl_loss = 0\n",
    "    for i, peer_output in enumerate(peer_outputs):\n",
    "        kl_loss += weights[i] * F.kl_div(\n",
    "            F.log_softmax(outputs, dim=-1),\n",
    "            F.softmax(peer_output, dim=-1),\n",
    "            reduction='batchmean'\n",
    "        )\n",
    "    \n",
    "    # Combine losses\n",
    "    loss = (1 - alpha) * ce_loss + alpha * kl_loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_peer_weights(model, peer_models, val_loader, current_weights, learning_rate, device):\n",
    "    model.eval()\n",
    "    for peer in peer_models:\n",
    "        peer.eval()\n",
    "    \n",
    "    gradients = torch.zeros_like(current_weights)\n",
    "    \n",
    "    for batch in tqdm(val_loader):\n",
    "        inputs = batch[0][0].squeeze(1).to(device) #need to select logits have highest dimension (when MRL is enabled)\n",
    "        labels = batch[1][0].squeeze(1).to(device)\n",
    "        \n",
    "        with torch.cuda.amp.autocast():\n",
    "            ensemble_output = sum(w * peer(inputs)[0][0].detach() for w, peer in zip(current_weights, peer_models))\n",
    "            loss = torch.nn.functional.cross_entropy(ensemble_output.view(-1, ensemble_output.size(-1)), labels.squeeze().reshape(labels.shape[1], -1))\n",
    "            \n",
    "            for i, peer in enumerate(peer_models):\n",
    "                peer_output = peer(inputs)[0][0]\n",
    "                peer_loss = torch.nn.functional.cross_entropy(peer_output.view(-1, peer_output.size(-1)), labels.squeeze().reshape(labels.shape[1], -1))\n",
    "                grad = torch.autograd.grad(peer_loss, peer.parameters(), allow_unused=True)\n",
    "                gradients[i] += sum(g.norm() if g is not None else 0 for g in grad)\n",
    "    \n",
    "    # Mirror descent update\n",
    "    new_weights = current_weights * torch.exp(-learning_rate * gradients)\n",
    "    new_weights /= new_weights.sum()  # Normalize\n",
    "    \n",
    "    return new_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "import math\n",
    "\n",
    "class BinaryFileDataset(IterableDataset):\n",
    "    def __init__(self, file_path, block_size, model, device):\n",
    "        self.file_path = file_path\n",
    "        self.block_size = block_size\n",
    "        self.device = device\n",
    "        self.data = None\n",
    "        self.pretrained_model = model\n",
    "\n",
    "    def load_adjusted_memmap(self):\n",
    "        with open(self.file_path, 'rb') as f:\n",
    "            data_bytes = f.read()\n",
    "        trimmed_length = (len(data_bytes) // 2) * 2\n",
    "        trimmed_data = data_bytes[:trimmed_length]\n",
    "        data = np.frombuffer(trimmed_data, dtype=np.uint16)\n",
    "        return data\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.data = self.load_adjusted_memmap()\n",
    "        while True:\n",
    "            ix = torch.randint(len(self.data) - self.block_size, (1,))\n",
    "            x = torch.from_numpy(self.data[ix:ix+self.block_size].astype(np.int64))\n",
    "            with torch.no_grad():\n",
    "                self.pretrained_model.eval()\n",
    "                y, _ = self.pretrained_model(x.unsqueeze(0).to(self.device))\n",
    "            yield x, y\n",
    "            \n",
    "    def estimate_length(self):\n",
    "        return len(self.load_adjusted_memmap()) - self.block_size + 1\n",
    "\n",
    "def get_dataloader(split, model, device, batch_size=256, block_size=64):\n",
    "    if split == 'train':\n",
    "        file_path = os.path.join('/Users/krishnaiyer/generative-ai-research-babylm/data/processed/train_10M/processed_encoded_train.bin')\n",
    "    else:\n",
    "        file_path = os.path.join('/Users/krishnaiyer/generative-ai-research-babylm/data/processed/train_10M/processed_encoded_val.bin')\n",
    "    dataset = BinaryFileDataset(file_path, block_size, model, device)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "    # Estimate the number of batches\n",
    "    estimated_samples = dataset.estimate_length()\n",
    "    estimated_batches = math.ceil(estimated_samples / batch_size)\n",
    "\n",
    "    return dataloader,estimated_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/krishnaiyer/miniforge3/envs/baby-lm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters before pruning: 196,121,096\n",
      "Non-zero parameters before pruning: 196,121,096\n",
      "Total parameters after pruning: 196,121,096\n",
      "Non-zero parameters after pruning: 172,055,048\n",
      "Effectively pruned 24,066,048 parameters\n",
      "Effective pruning ratio: 12.27%\n",
      "Total parameters before pruning: 196,121,096\n",
      "Non-zero parameters before pruning: 172,055,048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/krishnaiyer/miniforge3/envs/baby-lm/lib/python3.10/site-packages/torch/_compile.py:24: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information\n",
      "  return torch._dynamo.disable(fn, recursive)(*args, **kwargs)\n",
      "/Users/krishnaiyer/miniforge3/envs/baby-lm/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:125: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters after pruning: 196,121,096\n",
      "Non-zero parameters after pruning: 169,502,984\n",
      "Effectively pruned 2,552,064 parameters\n",
      "Effective pruning ratio: 1.48%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running batches: 0it [00:00, ?it/s]/Users/krishnaiyer/miniforge3/envs/baby-lm/lib/python3.10/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "Running batches: 3it [00:21,  7.02s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 142\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m# Compose the configuration\u001b[39;00m\n\u001b[1;32m    140\u001b[0m cfg \u001b[38;5;241m=\u001b[39m compose(config_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblm-main.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 142\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 85\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, loss \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(losses):\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(losses) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m accumulation_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 85\u001b[0m         \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43maccumulation_steps\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     87\u001b[0m         scaler\u001b[38;5;241m.\u001b[39mscale(loss \u001b[38;5;241m/\u001b[39m accumulation_steps)\u001b[38;5;241m.\u001b[39mbackward(retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniforge3/envs/baby-lm/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/baby-lm/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import sys\n",
    "from pathlib import Path\n",
    "base_path = Path('.').resolve().parent\n",
    "sys.path.append(str(base_path))\n",
    "import babylm as blm\n",
    "import hydra\n",
    "from hydra import initialize, compose\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "def create_peer_model(base_model, args, prune_ratio):\n",
    "    peer = base_model\n",
    "    X, Y = blm.gpt_2.utils.get_batch(split='train',args=args)\n",
    "    return extended_snip_pruning_gpt2(peer,X,Y,prune_ratio,args.train.n_head)\n",
    "\n",
    "def main(args):\n",
    "    # Hyperparameters\n",
    "    num_peers = 2\n",
    "    prune_ratios = [0.2, 0.4]\n",
    "    alpha = 0.5\n",
    "    num_epochs = 10\n",
    "    learning_rate = 1e-4\n",
    "    weight_update_frequency = 100  # Update weights every 100 steps\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    batch_size = 256  # Reduced batch size\n",
    "    accumulation_steps = 4  # Gradient accumulation steps\n",
    "    max_sequence_length = 16  # Reduced sequence length\n",
    "\n",
    "    # Load base model and tokenizer\n",
    "    checkpoint_path = \"MRL_mean_loss_2000_ckpt.pt\"\n",
    "    vocab_size = blm.gpt_2.utils.get_vocab_size(args)\n",
    "    base_model = blm.eval.utils.load_checkpoint(args,checkpoint_path,vocab_size)\n",
    "\n",
    "    # Create peer models\n",
    "    peer_models = [create_peer_model(base_model, args, ratio).to(device) for ratio in prune_ratios]\n",
    "\n",
    "    # Initialize peer weights\n",
    "    peer_weights = torch.ones(num_peers, device=device) / num_peers\n",
    "\n",
    "    # You should replace this with your actual dataset\n",
    "    train_dataloader,num_batches_train =  get_dataloader('train', base_model, device)\n",
    "    val_dataloader,num_batches_val = get_dataloader('val', base_model, device)\n",
    "\n",
    "    # Optimizer and scheduler\n",
    "    optimizer = torch.optim.Adam(sum([list(model.parameters()) for model in peer_models], []), lr=learning_rate)\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=0.95)\n",
    "    scaler = GradScaler()  # For mixed precision training\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        for model in peer_models:\n",
    "            model.train()\n",
    "        \n",
    "        train_loss = 0\n",
    "        optimizer.zero_grad()\n",
    "        for step, batch in tqdm(enumerate(train_dataloader),desc=f\"Running batches\"):\n",
    "            if args.MRL.enable:\n",
    "                inputs = batch[0].to(device) \n",
    "                labels = batch[1][0].squeeze().reshape(batch_size, -1).to(device) #need to select logits have highest dimension (when MRL is enabled)\n",
    "            else:\n",
    "                inputs = batch[0].squeeze(1).to(device) \n",
    "                labels = batch[1].squeeze().reshape(batch_size, -1).to(device) \n",
    "            \n",
    "            with autocast():\n",
    "                # Forward pass for all peers\n",
    "                if args.MRL.enable:\n",
    "                    peer_outputs = [model(inputs)[0][0] for model in peer_models]\n",
    "                else:\n",
    "                    peer_outputs = [model(inputs)[0] for model in peer_models]\n",
    "                \n",
    "                # Compute loss for each peer\n",
    "                losses = [wml_loss(outputs, labels, peer_outputs[:i] + peer_outputs[i+1:], \n",
    "                                   torch.cat([peer_weights[:i], peer_weights[i+1:]]), alpha) \n",
    "                          for i, outputs in enumerate(peer_outputs)]\n",
    "                \n",
    "                total_loss = sum(losses) / accumulation_steps\n",
    "\n",
    "            # Backward pass\n",
    "            for i, loss in enumerate(losses):\n",
    "                if i == len(losses) - 1 and (step + 1) % accumulation_steps == 0:\n",
    "                    scaler.scale(loss / accumulation_steps).backward()\n",
    "                else:\n",
    "                    scaler.scale(loss / accumulation_steps).backward(retain_graph=True)\n",
    "            \n",
    "            if (step + 1) % accumulation_steps == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            train_loss += total_loss.item()\n",
    "\n",
    "            # Update peer weights\n",
    "            if (step+1) % weight_update_frequency == 0:\n",
    "                peer_weights = update_peer_weights(base_model, peer_models, val_dataloader, peer_weights, learning_rate, device)\n",
    "\n",
    "            train_loss /= num_batches_train * num_peers\n",
    "\n",
    "            # Update peer weights\n",
    "            if (step+1) % weight_update_frequency == 0:\n",
    "                peer_weights = update_peer_weights(base_model, peer_models, val_dataloader, peer_weights, learning_rate, device)\n",
    "\n",
    "        train_loss /= num_batches_train * num_peers\n",
    "\n",
    "        # Validation\n",
    "        for model in peer_models:\n",
    "            model.eval()\n",
    "        \n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_dataloader:\n",
    "                inputs = batch[0].to(device)\n",
    "                labels = inputs.clone()\n",
    "                if args.MRL.enable:\n",
    "                    outputs = sum(w * model(inputs)[0][0] for w, model in zip(peer_weights, peer_models))\n",
    "                else:\n",
    "                    outputs = sum(w * model(inputs)[0] for w, model in zip(peer_weights, peer_models))\n",
    "                val_loss += torch.nn.functional.cross_entropy(outputs.view(-1, outputs.size(-1)), labels.view(-1)).item()\n",
    "        \n",
    "        val_loss /= num_batches_val\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} completed. Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "        \n",
    "        # Step the learning rate scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "    # Output final models and weights\n",
    "    for i, (model, weight) in enumerate(zip(peer_models, peer_weights)):\n",
    "        print(f\"Peer {i+1} weight: {weight.item():.4f}\")\n",
    "        model.save_pretrained(f\"/Users/krishnaiyer/generative-ai-research-babylm/models/WML/peer_model_{i+1}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize Hydra\n",
    "    initialize(version_base=None, config_path=\"../conf\")\n",
    "\n",
    "    # Compose the configuration\n",
    "    cfg = compose(config_name=\"blm-main.yaml\")\n",
    "\n",
    "    main(cfg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baby-lm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
