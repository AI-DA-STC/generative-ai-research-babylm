{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weighted Mutual Learning (WML), an alternative to distillation ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Pruning function : \n",
    "This function does the following:\n",
    "\n",
    "1. Calculates filter importance using the SNIP method.\n",
    "2. Computes layer importance based on average filter importance.\n",
    "3. Normalizes layer importance.\n",
    "4. Determines the number of parameters to prune per layer.\n",
    "5. Prunes filters in each layer based on their importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extended_snip_pruning_gpt2(model, X, Y, pruning_ratio, n_head, layer_bound=0.9):\n",
    "    # Enable gradients for all parameters\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    # Count non-zero parameters before pruning\n",
    "    total_params_before = sum(p.numel() for p in model.parameters())\n",
    "    non_zero_params_before = sum((p != 0).sum().item() for p in model.parameters())\n",
    "    print(f\"Total parameters before pruning: {total_params_before:,}\")\n",
    "    print(f\"Non-zero parameters before pruning: {non_zero_params_before:,}\")\n",
    "\n",
    "    # Forward pass\n",
    "    loss = model(X, Y)\n",
    "\n",
    "    # Backward pass\n",
    "    loss[1].backward()\n",
    "\n",
    "    # Calculate importance for attention heads and feed-forward layers\n",
    "    attention_importance = {}\n",
    "    ffn_importance = {}\n",
    "    \n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            if \"attn.c_attn\" in name:\n",
    "                importance = torch.sum(torch.abs(module.weight.grad * module.weight), dim=1)\n",
    "                attention_importance[name] = importance.view(3, n_head, -1).sum(dim=(0, 2))\n",
    "            elif \"attn.c_proj\" in name:\n",
    "                importance = torch.sum(torch.abs(module.weight.grad * module.weight), dim=0)\n",
    "                attention_importance[name] = importance.view(n_head, -1).sum(dim=1)\n",
    "            elif \"mlp.c_fc\" in name:\n",
    "                importance = torch.sum(torch.abs(module.weight.grad * module.weight), dim=1)\n",
    "                ffn_importance[name] = importance\n",
    "            elif \"mlp.c_proj\" in name:\n",
    "                importance = torch.sum(torch.abs(module.weight.grad * module.weight), dim=0)\n",
    "                ffn_importance[name] = importance\n",
    "\n",
    "    # Calculate layer importance\n",
    "    layer_importance = {}\n",
    "    for name, importance in attention_importance.items():\n",
    "        layer_name = name.rsplit(\".\", 2)[0]\n",
    "        layer_importance[layer_name] = torch.mean(importance)\n",
    "    \n",
    "    for name, importance in ffn_importance.items():\n",
    "        layer_name = name.rsplit(\".\", 2)[0]\n",
    "        layer_importance[layer_name] = (layer_importance.get(layer_name, 0) + torch.mean(importance)) / 2\n",
    "\n",
    "    # Normalize layer importance\n",
    "    total_importance = sum(layer_importance.values())\n",
    "    normalized_importance = {name: imp / total_importance for name, imp in layer_importance.items()}\n",
    "\n",
    "    # Calculate number of elements to prune per layer\n",
    "    total_elements = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    elements_to_prune = int(pruning_ratio * total_elements)\n",
    "    layer_prune_elements = {name: int(imp * elements_to_prune) for name, imp in normalized_importance.items()}\n",
    "\n",
    "    # Prune attention heads and feed-forward neurons\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            layer_name = name.rsplit(\".\", 2)[0]\n",
    "            if \"attn.c_attn\" in name:\n",
    "                head_importance = attention_importance[name]\n",
    "                heads_to_keep = max(1, int((1 - layer_bound) * n_head))\n",
    "                _, indices = torch.topk(head_importance, k=heads_to_keep, largest=True)\n",
    "                mask = torch.zeros(n_head, device=module.weight.device)\n",
    "                mask[indices] = 1\n",
    "                mask = mask.repeat_interleave(module.weight.size(0) // n_head).unsqueeze(1).expand_as(module.weight)\n",
    "                module.weight.data *= mask\n",
    "                if module.bias is not None:\n",
    "                    module.bias.data *= mask.squeeze()\n",
    "            elif \"attn.c_proj\" in name:\n",
    "                head_importance = attention_importance[name]\n",
    "                heads_to_keep = max(1, int((1 - layer_bound) * n_head))\n",
    "                _, indices = torch.topk(head_importance, k=heads_to_keep, largest=True)\n",
    "                mask = torch.zeros(n_head, device=module.weight.device)\n",
    "                mask[indices] = 1\n",
    "                mask = mask.repeat_interleave(module.weight.size(1) // n_head).unsqueeze(0).expand_as(module.weight)\n",
    "                module.weight.data *= mask\n",
    "            elif \"mlp.c_fc\" in name:\n",
    "                neuron_importance = ffn_importance[name]\n",
    "                neurons_to_keep = max(1, int((1 - layer_bound) * module.out_features))\n",
    "                _, indices = torch.topk(neuron_importance, k=neurons_to_keep, largest=True)\n",
    "                mask = torch.zeros(module.out_features, device=module.weight.device)\n",
    "                mask[indices] = 1\n",
    "                module.weight.data *= mask.unsqueeze(1).expand_as(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    module.bias.data *= mask\n",
    "\n",
    "            elif \"mlp.c_proj\" in name:\n",
    "                neuron_importance = ffn_importance[name]\n",
    "                neurons_to_keep = max(1, int((1 - layer_bound) * module.in_features))\n",
    "                _, indices = torch.topk(neuron_importance, k=neurons_to_keep, largest=True)\n",
    "                mask = torch.zeros(module.in_features, device=module.weight.device)\n",
    "                mask[indices] = 1\n",
    "                module.weight.data *= mask.unsqueeze(0).expand_as(module.weight)\n",
    "\n",
    "    # Reset gradients\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Count non-zero parameters after pruning\n",
    "    total_params_after = sum(p.numel() for p in model.parameters())\n",
    "    non_zero_params_after = sum((p != 0).sum().item() for p in model.parameters())\n",
    "    print(f\"Total parameters after pruning: {total_params_after:,}\")\n",
    "    print(f\"Non-zero parameters after pruning: {non_zero_params_after:,}\")\n",
    "    print(f\"Effectively pruned {non_zero_params_before - non_zero_params_after:,} parameters\")\n",
    "    print(f\"Effective pruning ratio: {(non_zero_params_before - non_zero_params_after) / non_zero_params_before:.2%}\")\n",
    "\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def wml_loss(outputs, labels, peer_outputs, weights, alpha):\n",
    "    \"\"\"\n",
    "    Compute the Weighted Mutual Learning loss for GPT-2.\n",
    "    \n",
    "    :param outputs: Logits from the current peer model\n",
    "    :param labels: True labels (input_ids for GPT-2)\n",
    "    :param peer_outputs: List of logits from other peer models\n",
    "    :param weights: Weights for each peer model\n",
    "    :param alpha: Balancing factor between CE loss and KL divergence\n",
    "    \"\"\"\n",
    "    # Cross-entropy loss\n",
    "    ce_loss = F.cross_entropy(outputs.squeeze(1), labels)\n",
    "    \n",
    "    # KL divergence loss\n",
    "    kl_loss = 0\n",
    "    for i, peer_output in enumerate(peer_outputs):\n",
    "        kl_loss += weights[i] * F.kl_div(\n",
    "            F.log_softmax(outputs, dim=-1),\n",
    "            F.softmax(peer_output, dim=-1),\n",
    "            reduction='batchmean'\n",
    "        )\n",
    "    \n",
    "    # Combine losses\n",
    "    loss = (1 - alpha) * ce_loss + alpha * kl_loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_peer_weights(model, peer_models, val_loader, current_weights, learning_rate, device):\n",
    "    model.eval()\n",
    "    for peer in peer_models:\n",
    "        peer.eval()\n",
    "    \n",
    "    gradients = torch.zeros_like(current_weights)\n",
    "    \n",
    "    for batch in tqdm(val_loader):\n",
    "        inputs = batch[0][0].squeeze(1).to(device) #need to select logits have highest dimension (when MRL is enabled)\n",
    "        labels = batch[1][0].squeeze(1).to(device)\n",
    "        \n",
    "        with torch.cuda.amp.autocast():\n",
    "            ensemble_output = sum(w * peer(inputs)[0][0].detach() for w, peer in zip(current_weights, peer_models))\n",
    "            loss = torch.nn.functional.cross_entropy(ensemble_output.view(-1, ensemble_output.size(-1)), labels.squeeze().reshape(labels.shape[1], -1))\n",
    "            \n",
    "            for i, peer in enumerate(peer_models):\n",
    "                peer_output = peer(inputs)[0][0]\n",
    "                peer_loss = torch.nn.functional.cross_entropy(peer_output.view(-1, peer_output.size(-1)), labels.squeeze().reshape(labels.shape[1], -1))\n",
    "                grad = torch.autograd.grad(peer_loss, peer.parameters(), allow_unused=True)\n",
    "                gradients[i] += sum(g.norm() if g is not None else 0 for g in grad)\n",
    "    \n",
    "    # Mirror descent update\n",
    "    new_weights = current_weights * torch.exp(-learning_rate * gradients)\n",
    "    new_weights /= new_weights.sum()  # Normalize\n",
    "    \n",
    "    return new_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "import math\n",
    "\n",
    "class BinaryFileDataset(IterableDataset):\n",
    "    def __init__(self, file_path, block_size, model, device):\n",
    "        self.file_path = file_path\n",
    "        self.block_size = block_size\n",
    "        self.device = device\n",
    "        self.data = None\n",
    "        self.pretrained_model = model\n",
    "\n",
    "    def load_adjusted_memmap(self):\n",
    "        with open(self.file_path, 'rb') as f:\n",
    "            data_bytes = f.read()\n",
    "        trimmed_length = (len(data_bytes) // 2) * 2\n",
    "        trimmed_data = data_bytes[:trimmed_length]\n",
    "        data = np.frombuffer(trimmed_data, dtype=np.uint16)\n",
    "        return data\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.data = self.load_adjusted_memmap()\n",
    "        while True:\n",
    "            ix = torch.randint(len(self.data) - self.block_size, (1,))\n",
    "            x = torch.from_numpy(self.data[ix:ix+self.block_size].astype(np.int64))\n",
    "            with torch.no_grad():\n",
    "                self.pretrained_model.eval()\n",
    "                y, _ = self.pretrained_model(x.unsqueeze(0).to(self.device))\n",
    "            yield x, y\n",
    "            \n",
    "    def estimate_length(self):\n",
    "        return len(self.load_adjusted_memmap()) - self.block_size + 1\n",
    "\n",
    "def get_dataloader(split, model, device, batch_size=256, block_size=64):\n",
    "    if split == 'train':\n",
    "        file_path = os.path.join('/Users/krishnaiyer/generative-ai-research-babylm/data/processed/train_10M/processed_encoded_train.bin')\n",
    "    else:\n",
    "        file_path = os.path.join('/Users/krishnaiyer/generative-ai-research-babylm/data/processed/train_10M/processed_encoded_val.bin')\n",
    "    dataset = BinaryFileDataset(file_path, block_size, model, device)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "    # Estimate the number of batches\n",
    "    estimated_samples = dataset.estimate_length()\n",
    "    estimated_batches = math.ceil(estimated_samples / batch_size)\n",
    "\n",
    "    return dataloader,estimated_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/krishnaiyer/miniforge3/envs/baby-lm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters before pruning: 196,121,096\n",
      "Non-zero parameters before pruning: 196,121,096\n",
      "Total parameters after pruning: 196,121,096\n",
      "Non-zero parameters after pruning: 172,055,048\n",
      "Effectively pruned 24,066,048 parameters\n",
      "Effective pruning ratio: 12.27%\n",
      "Total parameters before pruning: 196,121,096\n",
      "Non-zero parameters before pruning: 172,055,048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/krishnaiyer/miniforge3/envs/baby-lm/lib/python3.10/site-packages/torch/_compile.py:24: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information\n",
      "  return torch._dynamo.disable(fn, recursive)(*args, **kwargs)\n",
      "/Users/krishnaiyer/miniforge3/envs/baby-lm/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:125: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters after pruning: 196,121,096\n",
      "Non-zero parameters after pruning: 169,502,984\n",
      "Effectively pruned 2,552,064 parameters\n",
      "Effective pruning ratio: 1.48%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running batches: 0it [00:00, ?it/s]/Users/krishnaiyer/miniforge3/envs/baby-lm/lib/python3.10/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "Running batches: 3it [00:21,  7.02s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 142\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m# Compose the configuration\u001b[39;00m\n\u001b[1;32m    140\u001b[0m cfg \u001b[38;5;241m=\u001b[39m compose(config_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblm-main.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 142\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 85\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, loss \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(losses):\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(losses) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m accumulation_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 85\u001b[0m         \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43maccumulation_steps\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     87\u001b[0m         scaler\u001b[38;5;241m.\u001b[39mscale(loss \u001b[38;5;241m/\u001b[39m accumulation_steps)\u001b[38;5;241m.\u001b[39mbackward(retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniforge3/envs/baby-lm/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/baby-lm/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import sys\n",
    "from pathlib import Path\n",
    "base_path = Path('.').resolve().parent\n",
    "sys.path.append(str(base_path))\n",
    "import babylm as blm\n",
    "import hydra\n",
    "from hydra import initialize, compose\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "def create_peer_model(base_model, args, prune_ratio):\n",
    "    peer = base_model\n",
    "    X, Y = blm.gpt_2.utils.get_batch(split='train',args=args)\n",
    "    return extended_snip_pruning_gpt2(peer,X,Y,prune_ratio,args.train.n_head)\n",
    "\n",
    "def main(args):\n",
    "    # Hyperparameters\n",
    "    num_peers = 2\n",
    "    prune_ratios = [0.2, 0.4]\n",
    "    alpha = 0.5\n",
    "    num_epochs = 10\n",
    "    learning_rate = 1e-4\n",
    "    weight_update_frequency = 100  # Update weights every 100 steps\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    batch_size = 256  # Reduced batch size\n",
    "    accumulation_steps = 4  # Gradient accumulation steps\n",
    "    max_sequence_length = 16  # Reduced sequence length\n",
    "\n",
    "    # Load base model and tokenizer\n",
    "    checkpoint_path = \"MRL_mean_loss_2000_ckpt.pt\"\n",
    "    vocab_size = blm.gpt_2.utils.get_vocab_size(args)\n",
    "    base_model = blm.eval.utils.load_checkpoint(args,checkpoint_path,vocab_size)\n",
    "\n",
    "    # Create peer models\n",
    "    peer_models = [create_peer_model(base_model, args, ratio).to(device) for ratio in prune_ratios]\n",
    "\n",
    "    # Initialize peer weights\n",
    "    peer_weights = torch.ones(num_peers, device=device) / num_peers\n",
    "\n",
    "    # You should replace this with your actual dataset\n",
    "    train_dataloader,num_batches_train =  get_dataloader('train', base_model, device)\n",
    "    val_dataloader,num_batches_val = get_dataloader('val', base_model, device)\n",
    "\n",
    "    # Optimizer and scheduler\n",
    "    optimizer = torch.optim.Adam(sum([list(model.parameters()) for model in peer_models], []), lr=learning_rate)\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=0.95)\n",
    "    scaler = GradScaler()  # For mixed precision training\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        for model in peer_models:\n",
    "            model.train()\n",
    "        \n",
    "        train_loss = 0\n",
    "        optimizer.zero_grad()\n",
    "        for step, batch in tqdm(enumerate(train_dataloader),desc=f\"Running batches\"):\n",
    "            if args.MRL.enable:\n",
    "                inputs = batch[0].to(device) \n",
    "                labels = batch[1][0].squeeze().reshape(batch_size, -1).to(device) #need to select logits have highest dimension (when MRL is enabled)\n",
    "            else:\n",
    "                inputs = batch[0].squeeze(1).to(device) \n",
    "                labels = batch[1].squeeze().reshape(batch_size, -1).to(device) \n",
    "            \n",
    "            with autocast():\n",
    "                # Forward pass for all peers\n",
    "                if args.MRL.enable:\n",
    "                    peer_outputs = [model(inputs)[0][0] for model in peer_models]\n",
    "                else:\n",
    "                    peer_outputs = [model(inputs)[0] for model in peer_models]\n",
    "                \n",
    "                # Compute loss for each peer\n",
    "                losses = [wml_loss(outputs, labels, peer_outputs[:i] + peer_outputs[i+1:], \n",
    "                                   torch.cat([peer_weights[:i], peer_weights[i+1:]]), alpha) \n",
    "                          for i, outputs in enumerate(peer_outputs)]\n",
    "                \n",
    "                total_loss = sum(losses) / accumulation_steps\n",
    "\n",
    "            # Backward pass\n",
    "            for i, loss in enumerate(losses):\n",
    "                if i == len(losses) - 1 and (step + 1) % accumulation_steps == 0:\n",
    "                    scaler.scale(loss / accumulation_steps).backward()\n",
    "                else:\n",
    "                    scaler.scale(loss / accumulation_steps).backward(retain_graph=True)\n",
    "            \n",
    "            if (step + 1) % accumulation_steps == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            train_loss += total_loss.item()\n",
    "\n",
    "            # Update peer weights\n",
    "            if (step+1) % weight_update_frequency == 0:\n",
    "                peer_weights = update_peer_weights(base_model, peer_models, val_dataloader, peer_weights, learning_rate, device)\n",
    "\n",
    "            train_loss /= num_batches_train * num_peers\n",
    "\n",
    "            # Update peer weights\n",
    "            if (step+1) % weight_update_frequency == 0:\n",
    "                peer_weights = update_peer_weights(base_model, peer_models, val_dataloader, peer_weights, learning_rate, device)\n",
    "\n",
    "        train_loss /= num_batches_train * num_peers\n",
    "\n",
    "        # Validation\n",
    "        for model in peer_models:\n",
    "            model.eval()\n",
    "        \n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_dataloader:\n",
    "                inputs = batch[0].to(device)\n",
    "                labels = inputs.clone()\n",
    "                if args.MRL.enable:\n",
    "                    outputs = sum(w * model(inputs)[0][0] for w, model in zip(peer_weights, peer_models))\n",
    "                else:\n",
    "                    outputs = sum(w * model(inputs)[0] for w, model in zip(peer_weights, peer_models))\n",
    "                val_loss += torch.nn.functional.cross_entropy(outputs.view(-1, outputs.size(-1)), labels.view(-1)).item()\n",
    "        \n",
    "        val_loss /= num_batches_val\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} completed. Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "        \n",
    "        # Step the learning rate scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "    # Output final models and weights\n",
    "    for i, (model, weight) in enumerate(zip(peer_models, peer_weights)):\n",
    "        print(f\"Peer {i+1} weight: {weight.item():.4f}\")\n",
    "        model.save_pretrained(f\"/Users/krishnaiyer/generative-ai-research-babylm/models/WML/peer_model_{i+1}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize Hydra\n",
    "    initialize(version_base=None, config_path=\"../conf\")\n",
    "\n",
    "    # Compose the configuration\n",
    "    cfg = compose(config_name=\"blm-main.yaml\")\n",
    "\n",
    "    main(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import prune\n",
    "import copy\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "import sys\n",
    "from pathlib import Path\n",
    "base_path = Path('.').resolve().parent\n",
    "sys.path.append(str(base_path))\n",
    "import babylm as blm\n",
    "\n",
    "\n",
    "def prune_gpt_model(base_model, amount=0.3, importance='l1'):\n",
    "    model = copy.deepcopy(base_model)\n",
    "\n",
    "    for module in model.modules():\n",
    "        print(module)\n",
    "\n",
    "    parameters_to_prune = []\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, blm.gpt_2.attention.CausalSelfAttention):\n",
    "            parameters_to_prune.extend([\n",
    "                (module.c_attn, 'weight'),\n",
    "                (module.c_proj, 'weight')\n",
    "            ])\n",
    "        elif isinstance(module, blm.gpt_2.elements.MLP):\n",
    "            parameters_to_prune.extend([\n",
    "                (module.c_fc, 'weight'),\n",
    "                (module.c_proj, 'weight')\n",
    "            ])\n",
    "    \n",
    "    print(parameters_to_prune)\n",
    "    # Count total parameters to be pruned\n",
    "    total_params_to_prune = sum(p.numel() for module, _ in parameters_to_prune for p in [getattr(module, _)])\n",
    "    print(f\"total params to prune {total_params_to_prune}\")\n",
    "    \n",
    "    # Count total model parameters\n",
    "    total_model_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"total_model_params {total_model_params}\")\n",
    "\n",
    "    # Adjust pruning amount to achieve desired overall sparsity\n",
    "    adjusted_amount = (amount * total_model_params) / total_params_to_prune\n",
    "    print(f\"adjusted_amount {adjusted_amount}\")\n",
    "    \n",
    "    # Select pruning method\n",
    "    if importance == 'l1':\n",
    "        prune_method = prune.L1Unstructured\n",
    "    elif importance == 'random':\n",
    "        prune_method = prune.RandomUnstructured\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported importance method. Choose 'l1' or 'random'.\")\n",
    "\n",
    "    # Apply global unstructured pruning\n",
    "    prune.global_unstructured(\n",
    "        parameters_to_prune,\n",
    "        pruning_method=prune_method,\n",
    "        amount=amount\n",
    "    )\n",
    "\n",
    "    # Make the pruning permanent\n",
    "    for module, _ in parameters_to_prune:\n",
    "        prune.remove(module, 'weight')\n",
    "\n",
    "    return model\n",
    "\n",
    "def print_gpt_sparsity(model):\n",
    "    total_params = 0\n",
    "    zero_params = 0\n",
    "\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, (blm.gpt_2.attention.CausalSelfAttention, blm.gpt_2.elements.MLP)):\n",
    "            for param_name, param in module.named_parameters():\n",
    "                if 'weight' in param_name:\n",
    "                    layer_total = param.nelement()\n",
    "                    layer_zero = torch.sum(param == 0).item()\n",
    "                    layer_sparsity = 100.0 * layer_zero / layer_total\n",
    "                    print(f\"{name}.{param_name}: {layer_sparsity:.2f}% sparsity\")\n",
    "                    total_params += layer_total\n",
    "                    zero_params += layer_zero\n",
    "    overall_sparsity = 100.0 * zero_params / total_params\n",
    "    print(f\"Overall model sparsity: {overall_sparsity:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from hydra import initialize, compose\n",
    "\n",
    "def main(args):\n",
    "    # Load base model and tokenizer\n",
    "    checkpoint_path = \"GPT2_MRL_500_ckpt.pt\"\n",
    "    vocab_size = blm.gpt_2.utils.get_vocab_size(args)\n",
    "    base_model = blm.eval.utils.load_checkpoint(args,checkpoint_path,vocab_size)\n",
    "    \n",
    "    #pruned_gpt_model = prune_gpt_model(base_model, amount=0.5, importance='l1')\n",
    "    #print_gpt_sparsity(pruned_gpt_model)    \n",
    "    return base_model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize Hydra\n",
    "    initialize(version_base=None, config_path=\"../conf\")\n",
    "\n",
    "    # Compose the configuration\n",
    "    cfg = compose(config_name=\"blm-main.yaml\")\n",
    "\n",
    "    base_model = main(cfg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = base_model.transformer.h[0]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.attn.n_embd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New method for pruning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.prune as prune\n",
    "import copy\n",
    "\n",
    "def prune_gpt_model(base_model, layers, num_heads, prune_ratio, importance='l1'):\n",
    "    \"\"\"\n",
    "    Prune a GPT-style PyTorch model, focusing on attention heads and MLP layers.\n",
    "    \n",
    "    Args:\n",
    "    - base_model (nn.Module): The GPT model to be pruned.\n",
    "    - layers (list): List of layer indices to prune.\n",
    "    - num_heads (int): Number of attention heads to select for pruning.\n",
    "    - prune_ratio (float): The ratio of parameters to prune (0.0 to 1.0).\n",
    "    - importance (str): The importance measure for pruning ('l1' or 'random').\n",
    "    \n",
    "    Returns:\n",
    "    - pruned_model (nn.Module): The pruned model.\n",
    "    \"\"\"\n",
    "    model = copy.deepcopy(base_model)\n",
    "    pruned_modules = set()\n",
    "    \n",
    "    if importance == 'l1':\n",
    "        prune_method = prune.L1Unstructured\n",
    "    elif importance == 'random':\n",
    "        prune_method = prune.RandomUnstructured\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported importance method. Choose 'l1' or 'random'.\")\n",
    "    \n",
    "    for layer_idx in layers:\n",
    "        layer = model.transformer.h[layer_idx]\n",
    "        \n",
    "        # Prune attention heads\n",
    "        attn = layer.attn\n",
    "        \n",
    "        # Prune query, key, value projections\n",
    "        prune.ln_structured(attn.c_attn, name='weight', amount=prune_ratio, \n",
    "                            n=1, dim=0)  # Prune across Q, K, V\n",
    "        pruned_modules.add(attn.c_attn)\n",
    "        \n",
    "        # Prune output projection\n",
    "        prune.ln_structured(attn.c_proj, name='weight', amount=prune_ratio, \n",
    "                            n=1, dim=1)\n",
    "        pruned_modules.add(attn.c_proj)\n",
    "        \n",
    "        # Prune MLP\n",
    "        mlp = layer.mlp\n",
    "        prune.l1_unstructured(mlp.c_fc, name='weight', amount=prune_ratio)\n",
    "        prune.l1_unstructured(mlp.c_proj, name='weight', amount=prune_ratio)\n",
    "        pruned_modules.add(mlp.c_fc)\n",
    "        pruned_modules.add(mlp.c_proj)\n",
    "    \n",
    "    # Make pruning permanent\n",
    "    for module in pruned_modules:\n",
    "        prune.remove(module, 'weight')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Usage example:\n",
    "# model = YourGPTModel()\n",
    "# pruned_model = prune_gpt_model(model, layers=[0, 1, 2], num_heads=4, prune_ratio=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_model = prune_gpt_model(base_model, layers=[0, 1, 2, 3], num_heads=4, prune_ratio=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pruning of specific heads using custom mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.prune as prune\n",
    "import copy\n",
    "\n",
    "def prune_gpt_model(base_model, config, importance='l1'):\n",
    "    \"\"\"\n",
    "    Prune a GPT-style PyTorch model, focusing on top important attention heads and MLP layers.\n",
    "    \n",
    "    Args:\n",
    "    - base_model (nn.Module): The GPT model to be pruned.\n",
    "    - layers (list): List of layer indices to prune.\n",
    "    - num_heads (int): Number of top important attention heads to prune.\n",
    "    - prune_ratio (float): The ratio of parameters to prune (0.0 to 1.0).\n",
    "    - importance (str): The importance measure for pruning ('l1' or 'l2').\n",
    "    \n",
    "    Returns:\n",
    "    - pruned_model (nn.Module): The pruned model.\n",
    "    \"\"\"\n",
    "    layers = [item[0] for item in config]       \n",
    "    num_heads = [item[1] for item in config]    \n",
    "    pruning_ratios = [item[2] for item in config]\n",
    "    \n",
    "    model = copy.deepcopy(base_model)\n",
    "    pruned_modules = set()\n",
    "\n",
    "    for layer_idx, num_head, prune_ratio in zip(layers,num_heads,pruning_ratios):\n",
    "        layer = model.transformer.h[layer_idx]\n",
    "        attn = layer.attn\n",
    "        \n",
    "        # Calculate head importance\n",
    "        weight = attn.c_attn.weight\n",
    "        head_size = int(attn.n_embd / attn.n_head)\n",
    "        num_heads_total = attn.n_head\n",
    "        head_importance = []\n",
    "        for i in range(num_heads_total):\n",
    "            start = i * head_size\n",
    "            end = (i + 1) * head_size\n",
    "            head_weights = weight[start:end, :]\n",
    "            if importance == 'l1':\n",
    "                head_imp = torch.norm(head_weights, p=1)\n",
    "            elif importance == 'l2':\n",
    "                head_imp = torch.norm(head_weights, p=2)\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported importance method. Choose 'l1' or 'l2'.\")\n",
    "            head_importance.append((i, head_imp.item()))\n",
    "        \n",
    "        # Sort heads by importance (descending) and select top num_heads\n",
    "        head_importance.sort(key=lambda x: x[1], reverse=True)\n",
    "        heads_to_prune = head_importance[:num_head]\n",
    "        \n",
    "        # Create pruning mask for attention\n",
    "        attn_mask = torch.ones_like(weight)\n",
    "        for head_idx, _ in heads_to_prune:\n",
    "            start = head_idx * head_size\n",
    "            end = (head_idx + 1) * head_size\n",
    "            attn_mask[start:end, :] = 0\n",
    "        \n",
    "        # Apply pruning to attention\n",
    "        prune.custom_from_mask(attn.c_attn, name='weight', mask=attn_mask)\n",
    "        pruned_modules.add(attn.c_attn)\n",
    "        \n",
    "        # Prune corresponding parts in the output projection\n",
    "        proj_mask = torch.ones_like(attn.c_proj.weight)\n",
    "        for head_idx, _ in heads_to_prune:\n",
    "            start = head_idx * head_size\n",
    "            end = (head_idx + 1) * head_size\n",
    "            proj_mask[:, start:end] = 0\n",
    "        prune.custom_from_mask(attn.c_proj, name='weight', mask=proj_mask)\n",
    "        pruned_modules.add(attn.c_proj)\n",
    "        \n",
    "        # Prune MLP\n",
    "        mlp = layer.mlp\n",
    "        prune.l1_unstructured(mlp.c_fc, name='weight', amount=prune_ratio)\n",
    "        prune.l1_unstructured(mlp.c_proj, name='weight', amount=prune_ratio)\n",
    "        pruned_modules.add(mlp.c_fc)\n",
    "        pruned_modules.add(mlp.c_proj)\n",
    "    \n",
    "    # Make pruning permanent\n",
    "    for module in pruned_modules:\n",
    "        prune.remove(module, 'weight')\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### peer model generator using bayesian opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bayesian-optimization\n",
      "  Downloading bayesian_optimization-1.5.1-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: colorama<0.5.0,>=0.4.6 in /Users/krishnaiyer/miniforge3/envs/baby-lm/lib/python3.10/site-packages (from bayesian-optimization) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.25 in /Users/krishnaiyer/miniforge3/envs/baby-lm/lib/python3.10/site-packages (from bayesian-optimization) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn<2.0.0,>=1.0.0 in /Users/krishnaiyer/miniforge3/envs/baby-lm/lib/python3.10/site-packages (from bayesian-optimization) (1.4.2)\n",
      "Requirement already satisfied: scipy<2.0.0,>=1.0.0 in /Users/krishnaiyer/miniforge3/envs/baby-lm/lib/python3.10/site-packages (from bayesian-optimization) (1.13.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/krishnaiyer/miniforge3/envs/baby-lm/lib/python3.10/site-packages (from scikit-learn<2.0.0,>=1.0.0->bayesian-optimization) (1.4.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/krishnaiyer/miniforge3/envs/baby-lm/lib/python3.10/site-packages (from scikit-learn<2.0.0,>=1.0.0->bayesian-optimization) (3.4.0)\n",
      "Downloading bayesian_optimization-1.5.1-py3-none-any.whl (28 kB)\n",
      "Installing collected packages: bayesian-optimization\n",
      "Successfully installed bayesian-optimization-1.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install bayesian-optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "from typing import List, Tuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from bayes_opt import BayesianOptimization\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeNode:\n",
    "    def __init__(self, layer_idx: int, num_heads: int):\n",
    "        self.layer_idx = layer_idx\n",
    "        self.num_heads = num_heads\n",
    "        self.left = None\n",
    "        self.right = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a binary tree\n",
    "def create_pruning_tree(num_layers: int, base_heads: int, max_depth: int = 6) -> TreeNode:\n",
    "    def create_node(layer_idx: int, num_heads: int, depth: int) -> TreeNode:\n",
    "        if layer_idx > num_layers or depth > max_depth:\n",
    "            return None\n",
    "        \n",
    "        node = TreeNode(layer_idx, num_heads)\n",
    "        \n",
    "        if layer_idx + 1 < num_layers:\n",
    "            node.left = create_node(layer_idx + 1, num_heads, depth + 1)\n",
    "            node.right = create_node(layer_idx + 1, max(1, num_heads // 2), depth + 1)\n",
    "        \n",
    "        return node\n",
    "    \n",
    "    return create_node(0, base_heads, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting graphviz\n",
      "  Downloading graphviz-0.20.3-py3-none-any.whl.metadata (12 kB)\n",
      "Downloading graphviz-0.20.3-py3-none-any.whl (47 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.1/47.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: graphviz\n",
      "Successfully installed graphviz-0.20.3\n"
     ]
    }
   ],
   "source": [
    "!pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_pruning_tree(root: TreeNode, filename: str = \"pruning_tree.png\"):\n",
    "    dot = Digraph(comment='Pruning Tree')\n",
    "    dot.attr(rankdir='TB', size='8,8')\n",
    "    \n",
    "    def add_nodes_edges(node: TreeNode):\n",
    "        if node:\n",
    "            node_id = f\"{node.layer_idx}_{node.num_heads}\"\n",
    "            dot.node(node_id, f\"Layer {node.layer_idx}\\nHeads {node.num_heads}\")\n",
    "            if node.left:\n",
    "                left_id = f\"{node.left.layer_idx}_{node.left.num_heads}\"\n",
    "                dot.edge(node_id, left_id, 'left')\n",
    "                add_nodes_edges(node.left)\n",
    "            if node.right:\n",
    "                right_id = f\"{node.right.layer_idx}_{node.right.num_heads}\"\n",
    "                dot.edge(node_id, right_id, 'right')\n",
    "                add_nodes_edges(node.right)\n",
    "    \n",
    "    add_nodes_edges(root)\n",
    "    dot.render(filename, view=True, format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotADirectoryError",
     "evalue": "[Errno 20] Not a directory: PosixPath('dot')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[141], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m pruning_tree \u001b[38;5;241m=\u001b[39m create_pruning_tree(\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mvisualize_pruning_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpruning_tree\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[140], line 19\u001b[0m, in \u001b[0;36mvisualize_pruning_tree\u001b[0;34m(root, filename)\u001b[0m\n\u001b[1;32m     16\u001b[0m             add_nodes_edges(node\u001b[38;5;241m.\u001b[39mright)\n\u001b[1;32m     18\u001b[0m add_nodes_edges(root)\n\u001b[0;32m---> 19\u001b[0m \u001b[43mdot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mview\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpng\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/baby-lm/lib/python3.10/site-packages/graphviz/_tools.py:171\u001b[0m, in \u001b[0;36mdeprecate_positional_args.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m     wanted \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    163\u001b[0m                        \u001b[38;5;28;01mfor\u001b[39;00m name, value \u001b[38;5;129;01min\u001b[39;00m deprecated\u001b[38;5;241m.\u001b[39mitems())\n\u001b[1;32m    164\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe signature of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m will be reduced\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    165\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msupported_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m positional args\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    166\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(supported)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: pass \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwanted\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    167\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m as keyword arg(s)\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    168\u001b[0m                   stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    169\u001b[0m                   category\u001b[38;5;241m=\u001b[39mcategory)\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/baby-lm/lib/python3.10/site-packages/graphviz/rendering.py:122\u001b[0m, in \u001b[0;36mRender.render\u001b[0;34m(self, filename, directory, view, cleanup, format, renderer, formatter, neato_no_op, quiet, quiet_view, outfile, engine, raise_if_result_exists, overwrite_source)\u001b[0m\n\u001b[1;32m    118\u001b[0m filepath \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave(filename, directory\u001b[38;5;241m=\u001b[39mdirectory, skip_existing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    120\u001b[0m args\u001b[38;5;241m.\u001b[39mappend(filepath)\n\u001b[0;32m--> 122\u001b[0m rendered \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_render\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cleanup:\n\u001b[1;32m    125\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdelete \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m, filepath)\n",
      "File \u001b[0;32m~/miniforge3/envs/baby-lm/lib/python3.10/site-packages/graphviz/_tools.py:171\u001b[0m, in \u001b[0;36mdeprecate_positional_args.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m     wanted \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    163\u001b[0m                        \u001b[38;5;28;01mfor\u001b[39;00m name, value \u001b[38;5;129;01min\u001b[39;00m deprecated\u001b[38;5;241m.\u001b[39mitems())\n\u001b[1;32m    164\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe signature of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m will be reduced\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    165\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msupported_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m positional args\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    166\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(supported)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: pass \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwanted\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    167\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m as keyword arg(s)\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    168\u001b[0m                   stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    169\u001b[0m                   category\u001b[38;5;241m=\u001b[39mcategory)\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/baby-lm/lib/python3.10/site-packages/graphviz/backend/rendering.py:326\u001b[0m, in \u001b[0;36mrender\u001b[0;34m(engine, format, filepath, renderer, formatter, neato_no_op, quiet, outfile, raise_if_result_exists, overwrite_filepath)\u001b[0m\n\u001b[1;32m    322\u001b[0m cmd \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m filepath \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwork around pytype false alarm\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 326\u001b[0m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mcwd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilepath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparts\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mquiet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquiet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mcapture_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m os\u001b[38;5;241m.\u001b[39mfspath(outfile)\n",
      "File \u001b[0;32m~/miniforge3/envs/baby-lm/lib/python3.10/site-packages/graphviz/backend/execute.py:78\u001b[0m, in \u001b[0;36mrun_check\u001b[0;34m(cmd, input_lines, encoding, quiet, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m         proc \u001b[38;5;241m=\u001b[39m _run_input_lines(cmd, input_lines, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m         proc \u001b[38;5;241m=\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39merrno \u001b[38;5;241m==\u001b[39m errno\u001b[38;5;241m.\u001b[39mENOENT:\n",
      "File \u001b[0;32m~/miniforge3/envs/baby-lm/lib/python3.10/subprocess.py:503\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    500\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstdout\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m PIPE\n\u001b[1;32m    501\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstderr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m PIPE\n\u001b[0;32m--> 503\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpopenargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[1;32m    504\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    505\u001b[0m         stdout, stderr \u001b[38;5;241m=\u001b[39m process\u001b[38;5;241m.\u001b[39mcommunicate(\u001b[38;5;28minput\u001b[39m, timeout\u001b[38;5;241m=\u001b[39mtimeout)\n",
      "File \u001b[0;32m~/miniforge3/envs/baby-lm/lib/python3.10/subprocess.py:971\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize)\u001b[0m\n\u001b[1;32m    967\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_mode:\n\u001b[1;32m    968\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[1;32m    969\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m--> 971\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m    981\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n\u001b[1;32m    982\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdin, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr)):\n",
      "File \u001b[0;32m~/miniforge3/envs/baby-lm/lib/python3.10/subprocess.py:1863\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session)\u001b[0m\n\u001b[1;32m   1861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errno_num \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1862\u001b[0m         err_msg \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mstrerror(errno_num)\n\u001b[0;32m-> 1863\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(errno_num, err_msg, err_filename)\n\u001b[1;32m   1864\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(err_msg)\n",
      "\u001b[0;31mNotADirectoryError\u001b[0m: [Errno 20] Not a directory: PosixPath('dot')"
     ]
    }
   ],
   "source": [
    "pruning_tree = create_pruning_tree(4, 4)\n",
    "visualize_pruning_tree(pruning_tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#traverse the binary tree and extract the config\n",
    "def generate_peer_model_config(root: TreeNode, prune_ratio: List) -> List[Tuple[int, int, float]]:\n",
    "    config = []\n",
    "    node = root\n",
    "    layer_idx = 0\n",
    "    while node and layer_idx < len(prune_ratio):\n",
    "        config.append((node.layer_idx, node.num_heads, prune_ratio[layer_idx]))\n",
    "        node = node.left if random.random() < 0.5 else node.right\n",
    "        layer_idx += 1\n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian optimisation on normalised max difference of sparsities between peer models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sparsity(pruned_model):\n",
    "    total_params = 0\n",
    "    zero_params = 0\n",
    "    for name, module in pruned_model.named_modules():\n",
    "        if isinstance(module, (blm.gpt_2.attention.CausalSelfAttention, blm.gpt_2.elements.MLP)):\n",
    "            for param_name, param in module.named_parameters():\n",
    "                if 'weight' in param_name:\n",
    "                    layer_total = param.nelement()\n",
    "                    layer_zero = torch.sum(param == 0).item()\n",
    "                    total_params += layer_total\n",
    "                    zero_params += layer_zero\n",
    "    return zero_params / total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_peer_models(base_model: nn.Module, num_peers: int, num_layers: int, base_heads: int) -> List[List[Tuple[int, int, float]]]:\n",
    "    pruning_tree = create_pruning_tree(num_layers, base_heads)\n",
    "    \n",
    "    def objective(**kwargs):\n",
    "        all_prune_ratios = [[kwargs[f'peer_{p}_layer_{l}'] for l in range(num_layers)] for p in range(num_peers)]\n",
    "        configs = [generate_peer_model_config(pruning_tree, prune_ratios) for prune_ratios in all_prune_ratios]\n",
    "        sparsities = []\n",
    "        for config in configs:\n",
    "            pruned_model = prune_gpt_model(base_model,config)\n",
    "            sparsities.append(compute_sparsity(pruned_model))\n",
    "        \n",
    "        # Calculate the sum of absolute differences between all pairs\n",
    "        diff_sum = sum(abs(s1 - s2) for s1, s2 in itertools.combinations(sparsities, 2))\n",
    "        \n",
    "        # Normalize by the number of pairs\n",
    "        normalized_diff = (2 / (num_peers * (num_peers - 1))) * diff_sum if num_peers > 1 else 0\n",
    "        \n",
    "        # Include average sparsity in the objective\n",
    "        avg_sparsity = sum(sparsities) / num_peers\n",
    "        \n",
    "        # Combine normalized difference and average sparsity\n",
    "        return normalized_diff\n",
    "    \n",
    "    pbounds = {f'peer_{p}_layer_{l}': (0.1, 0.5) for p in range(num_peers) for l in range(num_layers)}\n",
    "    \n",
    "    optimizer = BayesianOptimization(\n",
    "        f=objective,\n",
    "        pbounds=pbounds,  \n",
    "        random_state=1,\n",
    "    )\n",
    "    \n",
    "    optimizer.maximize(init_points=10, n_iter=100)\n",
    "    \n",
    "    best_prune_ratios = [[optimizer.max['params'][f'peer_{p}_layer_{l}'] for l in range(num_layers)] for p in range(num_peers)]\n",
    "    best_configs = [generate_peer_model_config(pruning_tree, prune_ratios) for prune_ratios in best_prune_ratios]\n",
    "    return best_configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | peer_0... | peer_0... | peer_0... | peer_0... | peer_1... | peer_1... | peer_1... | peer_1... | peer_2... | peer_2... | peer_2... | peer_2... |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m0.07029  \u001b[39m | \u001b[39m0.2668   \u001b[39m | \u001b[39m0.3881   \u001b[39m | \u001b[39m0.1      \u001b[39m | \u001b[39m0.2209   \u001b[39m | \u001b[39m0.1587   \u001b[39m | \u001b[39m0.1369   \u001b[39m | \u001b[39m0.1745   \u001b[39m | \u001b[39m0.2382   \u001b[39m | \u001b[39m0.2587   \u001b[39m | \u001b[39m0.3155   \u001b[39m | \u001b[39m0.2677   \u001b[39m | \u001b[39m0.3741   \u001b[39m |\n",
      "| \u001b[39m2        \u001b[39m | \u001b[39m0.0649   \u001b[39m | \u001b[39m0.1818   \u001b[39m | \u001b[39m0.4512   \u001b[39m | \u001b[39m0.111    \u001b[39m | \u001b[39m0.3682   \u001b[39m | \u001b[39m0.2669   \u001b[39m | \u001b[39m0.3235   \u001b[39m | \u001b[39m0.1562   \u001b[39m | \u001b[39m0.1792   \u001b[39m | \u001b[39m0.4203   \u001b[39m | \u001b[39m0.4873   \u001b[39m | \u001b[39m0.2254   \u001b[39m | \u001b[39m0.3769   \u001b[39m |\n",
      "| \u001b[39m3        \u001b[39m | \u001b[39m0.0271   \u001b[39m | \u001b[39m0.4506   \u001b[39m | \u001b[39m0.4578   \u001b[39m | \u001b[39m0.134    \u001b[39m | \u001b[39m0.1156   \u001b[39m | \u001b[39m0.1679   \u001b[39m | \u001b[39m0.4513   \u001b[39m | \u001b[39m0.1393   \u001b[39m | \u001b[39m0.2684   \u001b[39m | \u001b[39m0.4832   \u001b[39m | \u001b[39m0.3133   \u001b[39m | \u001b[39m0.3768   \u001b[39m | \u001b[39m0.2262   \u001b[39m |\n",
      "| \u001b[35m4        \u001b[39m | \u001b[35m0.07939  \u001b[39m | \u001b[35m0.3746   \u001b[39m | \u001b[35m0.4339   \u001b[39m | \u001b[35m0.1073   \u001b[39m | \u001b[35m0.4001   \u001b[39m | \u001b[35m0.4955   \u001b[39m | \u001b[35m0.3993   \u001b[39m | \u001b[35m0.2122   \u001b[39m | \u001b[35m0.4157   \u001b[39m | \u001b[35m0.1413   \u001b[39m | \u001b[35m0.2792   \u001b[39m | \u001b[35m0.4634   \u001b[39m | \u001b[35m0.2174   \u001b[39m |\n",
      "| \u001b[35m5        \u001b[39m | \u001b[35m0.08557  \u001b[39m | \u001b[35m0.2151   \u001b[39m | \u001b[35m0.152    \u001b[39m | \u001b[35m0.1077   \u001b[39m | \u001b[35m0.3715   \u001b[39m | \u001b[35m0.1847   \u001b[39m | \u001b[35m0.2062   \u001b[39m | \u001b[35m0.2966   \u001b[39m | \u001b[35m0.1213   \u001b[39m | \u001b[35m0.3296   \u001b[39m | \u001b[35m0.1587   \u001b[39m | \u001b[35m0.3357   \u001b[39m | \u001b[35m0.3799   \u001b[39m |\n",
      "| \u001b[35m6        \u001b[39m | \u001b[35m0.09144  \u001b[39m | \u001b[35m0.1409   \u001b[39m | \u001b[35m0.2656   \u001b[39m | \u001b[35m0.3778   \u001b[39m | \u001b[35m0.2657   \u001b[39m | \u001b[35m0.12     \u001b[39m | \u001b[35m0.3144   \u001b[39m | \u001b[35m0.3655   \u001b[39m | \u001b[35m0.306    \u001b[39m | \u001b[35m0.4778   \u001b[39m | \u001b[35m0.3346   \u001b[39m | \u001b[35m0.4614   \u001b[39m | \u001b[35m0.155    \u001b[39m |\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m0.08348  \u001b[39m | \u001b[39m0.1557   \u001b[39m | \u001b[39m0.423    \u001b[39m | \u001b[39m0.2591   \u001b[39m | \u001b[39m0.1661   \u001b[39m | \u001b[39m0.471    \u001b[39m | \u001b[39m0.2391   \u001b[39m | \u001b[39m0.4003   \u001b[39m | \u001b[39m0.3904   \u001b[39m | \u001b[39m0.4533   \u001b[39m | \u001b[39m0.3495   \u001b[39m | \u001b[39m0.4004   \u001b[39m | \u001b[39m0.2396   \u001b[39m |\n",
      "| \u001b[39m8        \u001b[39m | \u001b[39m0.04393  \u001b[39m | \u001b[39m0.208    \u001b[39m | \u001b[39m0.4584   \u001b[39m | \u001b[39m0.2712   \u001b[39m | \u001b[39m0.4859   \u001b[39m | \u001b[39m0.3654   \u001b[39m | \u001b[39m0.3487   \u001b[39m | \u001b[39m0.1459   \u001b[39m | \u001b[39m0.4798   \u001b[39m | \u001b[39m0.28     \u001b[39m | \u001b[39m0.3314   \u001b[39m | \u001b[39m0.2633   \u001b[39m | \u001b[39m0.1948   \u001b[39m |\n",
      "| \u001b[39m9        \u001b[39m | \u001b[39m0.03773  \u001b[39m | \u001b[39m0.4614   \u001b[39m | \u001b[39m0.3295   \u001b[39m | \u001b[39m0.1011   \u001b[39m | \u001b[39m0.3469   \u001b[39m | \u001b[39m0.2307   \u001b[39m | \u001b[39m0.3108   \u001b[39m | \u001b[39m0.4544   \u001b[39m | \u001b[39m0.2429   \u001b[39m | \u001b[39m0.4634   \u001b[39m | \u001b[39m0.3493   \u001b[39m | \u001b[39m0.1063   \u001b[39m | \u001b[39m0.4718   \u001b[39m |\n",
      "| \u001b[39m10       \u001b[39m | \u001b[39m0.05067  \u001b[39m | \u001b[39m0.3764   \u001b[39m | \u001b[39m0.4989   \u001b[39m | \u001b[39m0.1689   \u001b[39m | \u001b[39m0.1549   \u001b[39m | \u001b[39m0.473    \u001b[39m | \u001b[39m0.3787   \u001b[39m | \u001b[39m0.1264   \u001b[39m | \u001b[39m0.4022   \u001b[39m | \u001b[39m0.4016   \u001b[39m | \u001b[39m0.4692   \u001b[39m | \u001b[39m0.3846   \u001b[39m | \u001b[39m0.1497   \u001b[39m |\n",
      "| \u001b[39m11       \u001b[39m | \u001b[39m0.03511  \u001b[39m | \u001b[39m0.1406   \u001b[39m | \u001b[39m0.3074   \u001b[39m | \u001b[39m0.3434   \u001b[39m | \u001b[39m0.237    \u001b[39m | \u001b[39m0.2232   \u001b[39m | \u001b[39m0.2877   \u001b[39m | \u001b[39m0.3803   \u001b[39m | \u001b[39m0.3282   \u001b[39m | \u001b[39m0.4699   \u001b[39m | \u001b[39m0.3371   \u001b[39m | \u001b[39m0.4448   \u001b[39m | \u001b[39m0.1818   \u001b[39m |\n",
      "| \u001b[39m12       \u001b[39m | \u001b[39m0.0816   \u001b[39m | \u001b[39m0.2      \u001b[39m | \u001b[39m0.2785   \u001b[39m | \u001b[39m0.1345   \u001b[39m | \u001b[39m0.4138   \u001b[39m | \u001b[39m0.156    \u001b[39m | \u001b[39m0.1706   \u001b[39m | \u001b[39m0.2202   \u001b[39m | \u001b[39m0.2251   \u001b[39m | \u001b[39m0.4846   \u001b[39m | \u001b[39m0.4186   \u001b[39m | \u001b[39m0.2678   \u001b[39m | \u001b[39m0.3353   \u001b[39m |\n",
      "| \u001b[39m13       \u001b[39m | \u001b[39m0.0259   \u001b[39m | \u001b[39m0.222    \u001b[39m | \u001b[39m0.3129   \u001b[39m | \u001b[39m0.3063   \u001b[39m | \u001b[39m0.1174   \u001b[39m | \u001b[39m0.1057   \u001b[39m | \u001b[39m0.4174   \u001b[39m | \u001b[39m0.4404   \u001b[39m | \u001b[39m0.2764   \u001b[39m | \u001b[39m0.4068   \u001b[39m | \u001b[39m0.335    \u001b[39m | \u001b[39m0.4835   \u001b[39m | \u001b[39m0.1538   \u001b[39m |\n",
      "| \u001b[39m14       \u001b[39m | \u001b[39m0.08511  \u001b[39m | \u001b[39m0.1181   \u001b[39m | \u001b[39m0.1816   \u001b[39m | \u001b[39m0.4139   \u001b[39m | \u001b[39m0.3345   \u001b[39m | \u001b[39m0.3714   \u001b[39m | \u001b[39m0.1815   \u001b[39m | \u001b[39m0.4008   \u001b[39m | \u001b[39m0.4228   \u001b[39m | \u001b[39m0.399    \u001b[39m | \u001b[39m0.1276   \u001b[39m | \u001b[39m0.4778   \u001b[39m | \u001b[39m0.1064   \u001b[39m |\n",
      "| \u001b[39m15       \u001b[39m | \u001b[39m0.009654 \u001b[39m | \u001b[39m0.3409   \u001b[39m | \u001b[39m0.3866   \u001b[39m | \u001b[39m0.4519   \u001b[39m | \u001b[39m0.2861   \u001b[39m | \u001b[39m0.4455   \u001b[39m | \u001b[39m0.3069   \u001b[39m | \u001b[39m0.3601   \u001b[39m | \u001b[39m0.2661   \u001b[39m | \u001b[39m0.484    \u001b[39m | \u001b[39m0.3594   \u001b[39m | \u001b[39m0.306    \u001b[39m | \u001b[39m0.238    \u001b[39m |\n",
      "| \u001b[39m16       \u001b[39m | \u001b[39m0.06612  \u001b[39m | \u001b[39m0.1124   \u001b[39m | \u001b[39m0.3162   \u001b[39m | \u001b[39m0.1199   \u001b[39m | \u001b[39m0.2201   \u001b[39m | \u001b[39m0.4075   \u001b[39m | \u001b[39m0.4501   \u001b[39m | \u001b[39m0.1509   \u001b[39m | \u001b[39m0.2928   \u001b[39m | \u001b[39m0.2913   \u001b[39m | \u001b[39m0.2268   \u001b[39m | \u001b[39m0.4722   \u001b[39m | \u001b[39m0.2151   \u001b[39m |\n",
      "| \u001b[39m17       \u001b[39m | \u001b[39m0.01997  \u001b[39m | \u001b[39m0.3287   \u001b[39m | \u001b[39m0.385    \u001b[39m | \u001b[39m0.3777   \u001b[39m | \u001b[39m0.1752   \u001b[39m | \u001b[39m0.1113   \u001b[39m | \u001b[39m0.4778   \u001b[39m | \u001b[39m0.2906   \u001b[39m | \u001b[39m0.166    \u001b[39m | \u001b[39m0.3398   \u001b[39m | \u001b[39m0.493    \u001b[39m | \u001b[39m0.1911   \u001b[39m | \u001b[39m0.235    \u001b[39m |\n",
      "| \u001b[39m18       \u001b[39m | \u001b[39m0.04324  \u001b[39m | \u001b[39m0.293    \u001b[39m | \u001b[39m0.3821   \u001b[39m | \u001b[39m0.1523   \u001b[39m | \u001b[39m0.358    \u001b[39m | \u001b[39m0.343    \u001b[39m | \u001b[39m0.326    \u001b[39m | \u001b[39m0.407    \u001b[39m | \u001b[39m0.1151   \u001b[39m | \u001b[39m0.341    \u001b[39m | \u001b[39m0.2406   \u001b[39m | \u001b[39m0.1185   \u001b[39m | \u001b[39m0.3745   \u001b[39m |\n",
      "| \u001b[39m19       \u001b[39m | \u001b[39m0.02911  \u001b[39m | \u001b[39m0.2828   \u001b[39m | \u001b[39m0.3519   \u001b[39m | \u001b[39m0.2534   \u001b[39m | \u001b[39m0.4062   \u001b[39m | \u001b[39m0.3698   \u001b[39m | \u001b[39m0.2141   \u001b[39m | \u001b[39m0.1119   \u001b[39m | \u001b[39m0.4156   \u001b[39m | \u001b[39m0.1317   \u001b[39m | \u001b[39m0.3398   \u001b[39m | \u001b[39m0.4315   \u001b[39m | \u001b[39m0.3453   \u001b[39m |\n",
      "| \u001b[39m20       \u001b[39m | \u001b[39m0.02033  \u001b[39m | \u001b[39m0.1654   \u001b[39m | \u001b[39m0.4883   \u001b[39m | \u001b[39m0.405    \u001b[39m | \u001b[39m0.3195   \u001b[39m | \u001b[39m0.395    \u001b[39m | \u001b[39m0.1782   \u001b[39m | \u001b[39m0.4546   \u001b[39m | \u001b[39m0.4851   \u001b[39m | \u001b[39m0.3678   \u001b[39m | \u001b[39m0.167    \u001b[39m | \u001b[39m0.1743   \u001b[39m | \u001b[39m0.4959   \u001b[39m |\n",
      "| \u001b[39m21       \u001b[39m | \u001b[39m0.07248  \u001b[39m | \u001b[39m0.2172   \u001b[39m | \u001b[39m0.1349   \u001b[39m | \u001b[39m0.1854   \u001b[39m | \u001b[39m0.3433   \u001b[39m | \u001b[39m0.3829   \u001b[39m | \u001b[39m0.217    \u001b[39m | \u001b[39m0.4925   \u001b[39m | \u001b[39m0.1283   \u001b[39m | \u001b[39m0.2076   \u001b[39m | \u001b[39m0.4309   \u001b[39m | \u001b[39m0.1602   \u001b[39m | \u001b[39m0.1333   \u001b[39m |\n",
      "| \u001b[39m22       \u001b[39m | \u001b[39m0.02866  \u001b[39m | \u001b[39m0.2621   \u001b[39m | \u001b[39m0.4532   \u001b[39m | \u001b[39m0.2616   \u001b[39m | \u001b[39m0.4603   \u001b[39m | \u001b[39m0.1193   \u001b[39m | \u001b[39m0.4588   \u001b[39m | \u001b[39m0.4685   \u001b[39m | \u001b[39m0.2246   \u001b[39m | \u001b[39m0.3298   \u001b[39m | \u001b[39m0.3875   \u001b[39m | \u001b[39m0.4469   \u001b[39m | \u001b[39m0.14     \u001b[39m |\n",
      "| \u001b[35m23       \u001b[39m | \u001b[35m0.09781  \u001b[39m | \u001b[35m0.4624   \u001b[39m | \u001b[35m0.4976   \u001b[39m | \u001b[35m0.2184   \u001b[39m | \u001b[35m0.3332   \u001b[39m | \u001b[35m0.1246   \u001b[39m | \u001b[35m0.4902   \u001b[39m | \u001b[35m0.1167   \u001b[39m | \u001b[35m0.2377   \u001b[39m | \u001b[35m0.4592   \u001b[39m | \u001b[35m0.3878   \u001b[39m | \u001b[35m0.2574   \u001b[39m | \u001b[35m0.3075   \u001b[39m |\n",
      "| \u001b[39m24       \u001b[39m | \u001b[39m0.01137  \u001b[39m | \u001b[39m0.2034   \u001b[39m | \u001b[39m0.4158   \u001b[39m | \u001b[39m0.1283   \u001b[39m | \u001b[39m0.1904   \u001b[39m | \u001b[39m0.1163   \u001b[39m | \u001b[39m0.2997   \u001b[39m | \u001b[39m0.1492   \u001b[39m | \u001b[39m0.458    \u001b[39m | \u001b[39m0.2378   \u001b[39m | \u001b[39m0.4539   \u001b[39m | \u001b[39m0.2902   \u001b[39m | \u001b[39m0.1095   \u001b[39m |\n",
      "| \u001b[39m25       \u001b[39m | \u001b[39m0.04474  \u001b[39m | \u001b[39m0.191    \u001b[39m | \u001b[39m0.1515   \u001b[39m | \u001b[39m0.4259   \u001b[39m | \u001b[39m0.4916   \u001b[39m | \u001b[39m0.3371   \u001b[39m | \u001b[39m0.4127   \u001b[39m | \u001b[39m0.4983   \u001b[39m | \u001b[39m0.2871   \u001b[39m | \u001b[39m0.2699   \u001b[39m | \u001b[39m0.1184   \u001b[39m | \u001b[39m0.2138   \u001b[39m | \u001b[39m0.2805   \u001b[39m |\n",
      "| \u001b[39m26       \u001b[39m | \u001b[39m0.05707  \u001b[39m | \u001b[39m0.1256   \u001b[39m | \u001b[39m0.253    \u001b[39m | \u001b[39m0.2901   \u001b[39m | \u001b[39m0.4283   \u001b[39m | \u001b[39m0.145    \u001b[39m | \u001b[39m0.3955   \u001b[39m | \u001b[39m0.3799   \u001b[39m | \u001b[39m0.281    \u001b[39m | \u001b[39m0.4569   \u001b[39m | \u001b[39m0.1739   \u001b[39m | \u001b[39m0.4135   \u001b[39m | \u001b[39m0.3788   \u001b[39m |\n",
      "| \u001b[39m27       \u001b[39m | \u001b[39m0.0136   \u001b[39m | \u001b[39m0.4061   \u001b[39m | \u001b[39m0.2186   \u001b[39m | \u001b[39m0.1605   \u001b[39m | \u001b[39m0.4763   \u001b[39m | \u001b[39m0.3001   \u001b[39m | \u001b[39m0.231    \u001b[39m | \u001b[39m0.2129   \u001b[39m | \u001b[39m0.1874   \u001b[39m | \u001b[39m0.2708   \u001b[39m | \u001b[39m0.283    \u001b[39m | \u001b[39m0.1592   \u001b[39m | \u001b[39m0.2959   \u001b[39m |\n",
      "| \u001b[39m28       \u001b[39m | \u001b[39m0.07313  \u001b[39m | \u001b[39m0.141    \u001b[39m | \u001b[39m0.2305   \u001b[39m | \u001b[39m0.4907   \u001b[39m | \u001b[39m0.4624   \u001b[39m | \u001b[39m0.4708   \u001b[39m | \u001b[39m0.2437   \u001b[39m | \u001b[39m0.3734   \u001b[39m | \u001b[39m0.278    \u001b[39m | \u001b[39m0.1442   \u001b[39m | \u001b[39m0.3546   \u001b[39m | \u001b[39m0.325    \u001b[39m | \u001b[39m0.2179   \u001b[39m |\n",
      "| \u001b[39m29       \u001b[39m | \u001b[39m0.05834  \u001b[39m | \u001b[39m0.1405   \u001b[39m | \u001b[39m0.1796   \u001b[39m | \u001b[39m0.4858   \u001b[39m | \u001b[39m0.3448   \u001b[39m | \u001b[39m0.4472   \u001b[39m | \u001b[39m0.2177   \u001b[39m | \u001b[39m0.2442   \u001b[39m | \u001b[39m0.1642   \u001b[39m | \u001b[39m0.2287   \u001b[39m | \u001b[39m0.2134   \u001b[39m | \u001b[39m0.387    \u001b[39m | \u001b[39m0.2341   \u001b[39m |\n",
      "| \u001b[39m30       \u001b[39m | \u001b[39m0.04946  \u001b[39m | \u001b[39m0.2352   \u001b[39m | \u001b[39m0.4915   \u001b[39m | \u001b[39m0.2048   \u001b[39m | \u001b[39m0.394    \u001b[39m | \u001b[39m0.4429   \u001b[39m | \u001b[39m0.1775   \u001b[39m | \u001b[39m0.1165   \u001b[39m | \u001b[39m0.3702   \u001b[39m | \u001b[39m0.133    \u001b[39m | \u001b[39m0.269    \u001b[39m | \u001b[39m0.302    \u001b[39m | \u001b[39m0.1454   \u001b[39m |\n",
      "| \u001b[39m31       \u001b[39m | \u001b[39m0.06348  \u001b[39m | \u001b[39m0.1993   \u001b[39m | \u001b[39m0.4337   \u001b[39m | \u001b[39m0.263    \u001b[39m | \u001b[39m0.396    \u001b[39m | \u001b[39m0.4956   \u001b[39m | \u001b[39m0.2681   \u001b[39m | \u001b[39m0.3981   \u001b[39m | \u001b[39m0.4569   \u001b[39m | \u001b[39m0.1756   \u001b[39m | \u001b[39m0.1158   \u001b[39m | \u001b[39m0.229    \u001b[39m | \u001b[39m0.3395   \u001b[39m |\n",
      "| \u001b[39m32       \u001b[39m | \u001b[39m0.03824  \u001b[39m | \u001b[39m0.1021   \u001b[39m | \u001b[39m0.1545   \u001b[39m | \u001b[39m0.238    \u001b[39m | \u001b[39m0.4297   \u001b[39m | \u001b[39m0.2171   \u001b[39m | \u001b[39m0.4507   \u001b[39m | \u001b[39m0.3972   \u001b[39m | \u001b[39m0.2034   \u001b[39m | \u001b[39m0.3228   \u001b[39m | \u001b[39m0.2672   \u001b[39m | \u001b[39m0.3031   \u001b[39m | \u001b[39m0.239    \u001b[39m |\n",
      "| \u001b[39m33       \u001b[39m | \u001b[39m0.01132  \u001b[39m | \u001b[39m0.2532   \u001b[39m | \u001b[39m0.3179   \u001b[39m | \u001b[39m0.19     \u001b[39m | \u001b[39m0.4579   \u001b[39m | \u001b[39m0.2512   \u001b[39m | \u001b[39m0.3478   \u001b[39m | \u001b[39m0.281    \u001b[39m | \u001b[39m0.237    \u001b[39m | \u001b[39m0.3035   \u001b[39m | \u001b[39m0.4919   \u001b[39m | \u001b[39m0.3098   \u001b[39m | \u001b[39m0.1982   \u001b[39m |\n",
      "| \u001b[39m34       \u001b[39m | \u001b[39m0.05085  \u001b[39m | \u001b[39m0.3345   \u001b[39m | \u001b[39m0.3147   \u001b[39m | \u001b[39m0.4698   \u001b[39m | \u001b[39m0.372    \u001b[39m | \u001b[39m0.41     \u001b[39m | \u001b[39m0.4229   \u001b[39m | \u001b[39m0.2586   \u001b[39m | \u001b[39m0.36     \u001b[39m | \u001b[39m0.1544   \u001b[39m | \u001b[39m0.1892   \u001b[39m | \u001b[39m0.2785   \u001b[39m | \u001b[39m0.4968   \u001b[39m |\n",
      "| \u001b[39m35       \u001b[39m | \u001b[39m0.00256  \u001b[39m | \u001b[39m0.164    \u001b[39m | \u001b[39m0.1367   \u001b[39m | \u001b[39m0.1793   \u001b[39m | \u001b[39m0.3988   \u001b[39m | \u001b[39m0.3899   \u001b[39m | \u001b[39m0.134    \u001b[39m | \u001b[39m0.3072   \u001b[39m | \u001b[39m0.3206   \u001b[39m | \u001b[39m0.3357   \u001b[39m | \u001b[39m0.3567   \u001b[39m | \u001b[39m0.4741   \u001b[39m | \u001b[39m0.1064   \u001b[39m |\n",
      "| \u001b[39m36       \u001b[39m | \u001b[39m0.06909  \u001b[39m | \u001b[39m0.1512   \u001b[39m | \u001b[39m0.275    \u001b[39m | \u001b[39m0.2197   \u001b[39m | \u001b[39m0.4599   \u001b[39m | \u001b[39m0.3838   \u001b[39m | \u001b[39m0.1371   \u001b[39m | \u001b[39m0.1368   \u001b[39m | \u001b[39m0.3468   \u001b[39m | \u001b[39m0.3631   \u001b[39m | \u001b[39m0.3537   \u001b[39m | \u001b[39m0.3205   \u001b[39m | \u001b[39m0.464    \u001b[39m |\n",
      "| \u001b[35m37       \u001b[39m | \u001b[35m0.1075   \u001b[39m | \u001b[35m0.4477   \u001b[39m | \u001b[35m0.258    \u001b[39m | \u001b[35m0.3556   \u001b[39m | \u001b[35m0.435    \u001b[39m | \u001b[35m0.4731   \u001b[39m | \u001b[35m0.1881   \u001b[39m | \u001b[35m0.2731   \u001b[39m | \u001b[35m0.4667   \u001b[39m | \u001b[35m0.1794   \u001b[39m | \u001b[35m0.2806   \u001b[39m | \u001b[35m0.1737   \u001b[39m | \u001b[35m0.2706   \u001b[39m |\n",
      "| \u001b[35m38       \u001b[39m | \u001b[35m0.1249   \u001b[39m | \u001b[35m0.1095   \u001b[39m | \u001b[35m0.3892   \u001b[39m | \u001b[35m0.3308   \u001b[39m | \u001b[35m0.3272   \u001b[39m | \u001b[35m0.1913   \u001b[39m | \u001b[35m0.4868   \u001b[39m | \u001b[35m0.2257   \u001b[39m | \u001b[35m0.1498   \u001b[39m | \u001b[35m0.456    \u001b[39m | \u001b[35m0.3278   \u001b[39m | \u001b[35m0.4186   \u001b[39m | \u001b[35m0.4752   \u001b[39m |\n",
      "| \u001b[39m39       \u001b[39m | \u001b[39m0.08567  \u001b[39m | \u001b[39m0.1095   \u001b[39m | \u001b[39m0.3892   \u001b[39m | \u001b[39m0.3308   \u001b[39m | \u001b[39m0.3272   \u001b[39m | \u001b[39m0.1913   \u001b[39m | \u001b[39m0.4868   \u001b[39m | \u001b[39m0.2257   \u001b[39m | \u001b[39m0.1498   \u001b[39m | \u001b[39m0.456    \u001b[39m | \u001b[39m0.3278   \u001b[39m | \u001b[39m0.4186   \u001b[39m | \u001b[39m0.4752   \u001b[39m |\n",
      "| \u001b[39m40       \u001b[39m | \u001b[39m0.05185  \u001b[39m | \u001b[39m0.2224   \u001b[39m | \u001b[39m0.1921   \u001b[39m | \u001b[39m0.4407   \u001b[39m | \u001b[39m0.2853   \u001b[39m | \u001b[39m0.422    \u001b[39m | \u001b[39m0.3944   \u001b[39m | \u001b[39m0.4609   \u001b[39m | \u001b[39m0.2968   \u001b[39m | \u001b[39m0.3228   \u001b[39m | \u001b[39m0.1247   \u001b[39m | \u001b[39m0.172    \u001b[39m | \u001b[39m0.3004   \u001b[39m |\n",
      "| \u001b[39m41       \u001b[39m | \u001b[39m0.05471  \u001b[39m | \u001b[39m0.4419   \u001b[39m | \u001b[39m0.2096   \u001b[39m | \u001b[39m0.3862   \u001b[39m | \u001b[39m0.2994   \u001b[39m | \u001b[39m0.4123   \u001b[39m | \u001b[39m0.2823   \u001b[39m | \u001b[39m0.1257   \u001b[39m | \u001b[39m0.212    \u001b[39m | \u001b[39m0.4335   \u001b[39m | \u001b[39m0.1679   \u001b[39m | \u001b[39m0.4899   \u001b[39m | \u001b[39m0.1137   \u001b[39m |\n",
      "| \u001b[39m42       \u001b[39m | \u001b[39m0.08289  \u001b[39m | \u001b[39m0.2648   \u001b[39m | \u001b[39m0.1411   \u001b[39m | \u001b[39m0.2342   \u001b[39m | \u001b[39m0.272    \u001b[39m | \u001b[39m0.3527   \u001b[39m | \u001b[39m0.1224   \u001b[39m | \u001b[39m0.1165   \u001b[39m | \u001b[39m0.1635   \u001b[39m | \u001b[39m0.4135   \u001b[39m | \u001b[39m0.1      \u001b[39m | \u001b[39m0.2288   \u001b[39m | \u001b[39m0.4156   \u001b[39m |\n",
      "| \u001b[39m43       \u001b[39m | \u001b[39m0.05944  \u001b[39m | \u001b[39m0.1224   \u001b[39m | \u001b[39m0.393    \u001b[39m | \u001b[39m0.1801   \u001b[39m | \u001b[39m0.4556   \u001b[39m | \u001b[39m0.4009   \u001b[39m | \u001b[39m0.2851   \u001b[39m | \u001b[39m0.1302   \u001b[39m | \u001b[39m0.4431   \u001b[39m | \u001b[39m0.2391   \u001b[39m | \u001b[39m0.4022   \u001b[39m | \u001b[39m0.4271   \u001b[39m | \u001b[39m0.4301   \u001b[39m |\n",
      "| \u001b[39m44       \u001b[39m | \u001b[39m0.07838  \u001b[39m | \u001b[39m0.2243   \u001b[39m | \u001b[39m0.4589   \u001b[39m | \u001b[39m0.2558   \u001b[39m | \u001b[39m0.1999   \u001b[39m | \u001b[39m0.4325   \u001b[39m | \u001b[39m0.142    \u001b[39m | \u001b[39m0.4384   \u001b[39m | \u001b[39m0.2466   \u001b[39m | \u001b[39m0.4356   \u001b[39m | \u001b[39m0.2736   \u001b[39m | \u001b[39m0.2881   \u001b[39m | \u001b[39m0.472    \u001b[39m |\n",
      "| \u001b[39m45       \u001b[39m | \u001b[39m0.04186  \u001b[39m | \u001b[39m0.1747   \u001b[39m | \u001b[39m0.1094   \u001b[39m | \u001b[39m0.1539   \u001b[39m | \u001b[39m0.19     \u001b[39m | \u001b[39m0.1901   \u001b[39m | \u001b[39m0.2601   \u001b[39m | \u001b[39m0.3601   \u001b[39m | \u001b[39m0.3819   \u001b[39m | \u001b[39m0.1227   \u001b[39m | \u001b[39m0.1861   \u001b[39m | \u001b[39m0.176    \u001b[39m | \u001b[39m0.3379   \u001b[39m |\n",
      "| \u001b[39m46       \u001b[39m | \u001b[39m0.02192  \u001b[39m | \u001b[39m0.4848   \u001b[39m | \u001b[39m0.2132   \u001b[39m | \u001b[39m0.2834   \u001b[39m | \u001b[39m0.2614   \u001b[39m | \u001b[39m0.1351   \u001b[39m | \u001b[39m0.3649   \u001b[39m | \u001b[39m0.3067   \u001b[39m | \u001b[39m0.4262   \u001b[39m | \u001b[39m0.3196   \u001b[39m | \u001b[39m0.4243   \u001b[39m | \u001b[39m0.1075   \u001b[39m | \u001b[39m0.4648   \u001b[39m |\n",
      "| \u001b[39m47       \u001b[39m | \u001b[39m0.05645  \u001b[39m | \u001b[39m0.4157   \u001b[39m | \u001b[39m0.4595   \u001b[39m | \u001b[39m0.3604   \u001b[39m | \u001b[39m0.4342   \u001b[39m | \u001b[39m0.1711   \u001b[39m | \u001b[39m0.4277   \u001b[39m | \u001b[39m0.3763   \u001b[39m | \u001b[39m0.2491   \u001b[39m | \u001b[39m0.2271   \u001b[39m | \u001b[39m0.1263   \u001b[39m | \u001b[39m0.4019   \u001b[39m | \u001b[39m0.311    \u001b[39m |\n",
      "| \u001b[39m48       \u001b[39m | \u001b[39m0.08099  \u001b[39m | \u001b[39m0.2067   \u001b[39m | \u001b[39m0.4286   \u001b[39m | \u001b[39m0.2497   \u001b[39m | \u001b[39m0.3296   \u001b[39m | \u001b[39m0.4049   \u001b[39m | \u001b[39m0.3122   \u001b[39m | \u001b[39m0.25     \u001b[39m | \u001b[39m0.4764   \u001b[39m | \u001b[39m0.2459   \u001b[39m | \u001b[39m0.3296   \u001b[39m | \u001b[39m0.2225   \u001b[39m | \u001b[39m0.3869   \u001b[39m |\n",
      "| \u001b[39m49       \u001b[39m | \u001b[39m0.02403  \u001b[39m | \u001b[39m0.2176   \u001b[39m | \u001b[39m0.3649   \u001b[39m | \u001b[39m0.1412   \u001b[39m | \u001b[39m0.3621   \u001b[39m | \u001b[39m0.3136   \u001b[39m | \u001b[39m0.2683   \u001b[39m | \u001b[39m0.262    \u001b[39m | \u001b[39m0.4583   \u001b[39m | \u001b[39m0.3925   \u001b[39m | \u001b[39m0.1645   \u001b[39m | \u001b[39m0.3374   \u001b[39m | \u001b[39m0.3727   \u001b[39m |\n",
      "| \u001b[39m50       \u001b[39m | \u001b[39m0.0197   \u001b[39m | \u001b[39m0.1051   \u001b[39m | \u001b[39m0.265    \u001b[39m | \u001b[39m0.186    \u001b[39m | \u001b[39m0.1343   \u001b[39m | \u001b[39m0.3169   \u001b[39m | \u001b[39m0.2218   \u001b[39m | \u001b[39m0.2271   \u001b[39m | \u001b[39m0.2165   \u001b[39m | \u001b[39m0.314    \u001b[39m | \u001b[39m0.1928   \u001b[39m | \u001b[39m0.4119   \u001b[39m | \u001b[39m0.1989   \u001b[39m |\n",
      "| \u001b[39m51       \u001b[39m | \u001b[39m0.02239  \u001b[39m | \u001b[39m0.2821   \u001b[39m | \u001b[39m0.2346   \u001b[39m | \u001b[39m0.2521   \u001b[39m | \u001b[39m0.1039   \u001b[39m | \u001b[39m0.3541   \u001b[39m | \u001b[39m0.2767   \u001b[39m | \u001b[39m0.2107   \u001b[39m | \u001b[39m0.4281   \u001b[39m | \u001b[39m0.1905   \u001b[39m | \u001b[39m0.2118   \u001b[39m | \u001b[39m0.1383   \u001b[39m | \u001b[39m0.1525   \u001b[39m |\n",
      "| \u001b[39m52       \u001b[39m | \u001b[39m0.0278   \u001b[39m | \u001b[39m0.4261   \u001b[39m | \u001b[39m0.4086   \u001b[39m | \u001b[39m0.1393   \u001b[39m | \u001b[39m0.1244   \u001b[39m | \u001b[39m0.4447   \u001b[39m | \u001b[39m0.3595   \u001b[39m | \u001b[39m0.1279   \u001b[39m | \u001b[39m0.3597   \u001b[39m | \u001b[39m0.3576   \u001b[39m | \u001b[39m0.1384   \u001b[39m | \u001b[39m0.2092   \u001b[39m | \u001b[39m0.4619   \u001b[39m |\n",
      "| \u001b[39m53       \u001b[39m | \u001b[39m0.04157  \u001b[39m | \u001b[39m0.1095   \u001b[39m | \u001b[39m0.3892   \u001b[39m | \u001b[39m0.3308   \u001b[39m | \u001b[39m0.3272   \u001b[39m | \u001b[39m0.1913   \u001b[39m | \u001b[39m0.4868   \u001b[39m | \u001b[39m0.2257   \u001b[39m | \u001b[39m0.1498   \u001b[39m | \u001b[39m0.456    \u001b[39m | \u001b[39m0.3278   \u001b[39m | \u001b[39m0.4186   \u001b[39m | \u001b[39m0.4752   \u001b[39m |\n",
      "| \u001b[39m54       \u001b[39m | \u001b[39m0.1249   \u001b[39m | \u001b[39m0.1095   \u001b[39m | \u001b[39m0.3892   \u001b[39m | \u001b[39m0.3308   \u001b[39m | \u001b[39m0.3272   \u001b[39m | \u001b[39m0.1913   \u001b[39m | \u001b[39m0.4868   \u001b[39m | \u001b[39m0.2257   \u001b[39m | \u001b[39m0.1498   \u001b[39m | \u001b[39m0.456    \u001b[39m | \u001b[39m0.3278   \u001b[39m | \u001b[39m0.4186   \u001b[39m | \u001b[39m0.4752   \u001b[39m |\n",
      "| \u001b[39m55       \u001b[39m | \u001b[39m0.03804  \u001b[39m | \u001b[39m0.2644   \u001b[39m | \u001b[39m0.2099   \u001b[39m | \u001b[39m0.1725   \u001b[39m | \u001b[39m0.3448   \u001b[39m | \u001b[39m0.3738   \u001b[39m | \u001b[39m0.4852   \u001b[39m | \u001b[39m0.1781   \u001b[39m | \u001b[39m0.2226   \u001b[39m | \u001b[39m0.1301   \u001b[39m | \u001b[39m0.1142   \u001b[39m | \u001b[39m0.3213   \u001b[39m | \u001b[39m0.4143   \u001b[39m |\n",
      "| \u001b[39m56       \u001b[39m | \u001b[39m0.03173  \u001b[39m | \u001b[39m0.475    \u001b[39m | \u001b[39m0.4123   \u001b[39m | \u001b[39m0.1711   \u001b[39m | \u001b[39m0.458    \u001b[39m | \u001b[39m0.4839   \u001b[39m | \u001b[39m0.4296   \u001b[39m | \u001b[39m0.1903   \u001b[39m | \u001b[39m0.2589   \u001b[39m | \u001b[39m0.3439   \u001b[39m | \u001b[39m0.416    \u001b[39m | \u001b[39m0.1917   \u001b[39m | \u001b[39m0.1968   \u001b[39m |\n",
      "| \u001b[39m57       \u001b[39m | \u001b[39m0.04662  \u001b[39m | \u001b[39m0.3181   \u001b[39m | \u001b[39m0.31     \u001b[39m | \u001b[39m0.1215   \u001b[39m | \u001b[39m0.4103   \u001b[39m | \u001b[39m0.2483   \u001b[39m | \u001b[39m0.4405   \u001b[39m | \u001b[39m0.3946   \u001b[39m | \u001b[39m0.371    \u001b[39m | \u001b[39m0.2853   \u001b[39m | \u001b[39m0.138    \u001b[39m | \u001b[39m0.3699   \u001b[39m | \u001b[39m0.3046   \u001b[39m |\n",
      "| \u001b[39m58       \u001b[39m | \u001b[39m0.03359  \u001b[39m | \u001b[39m0.3558   \u001b[39m | \u001b[39m0.1208   \u001b[39m | \u001b[39m0.3137   \u001b[39m | \u001b[39m0.2556   \u001b[39m | \u001b[39m0.3798   \u001b[39m | \u001b[39m0.3125   \u001b[39m | \u001b[39m0.3111   \u001b[39m | \u001b[39m0.2443   \u001b[39m | \u001b[39m0.2414   \u001b[39m | \u001b[39m0.418    \u001b[39m | \u001b[39m0.1499   \u001b[39m | \u001b[39m0.2466   \u001b[39m |\n",
      "| \u001b[39m59       \u001b[39m | \u001b[39m0.03294  \u001b[39m | \u001b[39m0.3201   \u001b[39m | \u001b[39m0.4916   \u001b[39m | \u001b[39m0.2573   \u001b[39m | \u001b[39m0.13     \u001b[39m | \u001b[39m0.2308   \u001b[39m | \u001b[39m0.1826   \u001b[39m | \u001b[39m0.2793   \u001b[39m | \u001b[39m0.2099   \u001b[39m | \u001b[39m0.4929   \u001b[39m | \u001b[39m0.1026   \u001b[39m | \u001b[39m0.1063   \u001b[39m | \u001b[39m0.106    \u001b[39m |\n",
      "| \u001b[39m60       \u001b[39m | \u001b[39m0.1249   \u001b[39m | \u001b[39m0.1095   \u001b[39m | \u001b[39m0.3892   \u001b[39m | \u001b[39m0.3308   \u001b[39m | \u001b[39m0.3272   \u001b[39m | \u001b[39m0.1913   \u001b[39m | \u001b[39m0.4868   \u001b[39m | \u001b[39m0.2257   \u001b[39m | \u001b[39m0.1498   \u001b[39m | \u001b[39m0.456    \u001b[39m | \u001b[39m0.3278   \u001b[39m | \u001b[39m0.4186   \u001b[39m | \u001b[39m0.4752   \u001b[39m |\n",
      "| \u001b[39m61       \u001b[39m | \u001b[39m0.06132  \u001b[39m | \u001b[39m0.1426   \u001b[39m | \u001b[39m0.4813   \u001b[39m | \u001b[39m0.3673   \u001b[39m | \u001b[39m0.1619   \u001b[39m | \u001b[39m0.3527   \u001b[39m | \u001b[39m0.283    \u001b[39m | \u001b[39m0.4465   \u001b[39m | \u001b[39m0.1005   \u001b[39m | \u001b[39m0.2281   \u001b[39m | \u001b[39m0.1044   \u001b[39m | \u001b[39m0.1923   \u001b[39m | \u001b[39m0.106    \u001b[39m |\n",
      "| \u001b[39m62       \u001b[39m | \u001b[39m0.04608  \u001b[39m | \u001b[39m0.4009   \u001b[39m | \u001b[39m0.2536   \u001b[39m | \u001b[39m0.4582   \u001b[39m | \u001b[39m0.3295   \u001b[39m | \u001b[39m0.3532   \u001b[39m | \u001b[39m0.3289   \u001b[39m | \u001b[39m0.4162   \u001b[39m | \u001b[39m0.1516   \u001b[39m | \u001b[39m0.2157   \u001b[39m | \u001b[39m0.1665   \u001b[39m | \u001b[39m0.3079   \u001b[39m | \u001b[39m0.2748   \u001b[39m |\n",
      "| \u001b[39m63       \u001b[39m | \u001b[39m0.01414  \u001b[39m | \u001b[39m0.1628   \u001b[39m | \u001b[39m0.4777   \u001b[39m | \u001b[39m0.1535   \u001b[39m | \u001b[39m0.1582   \u001b[39m | \u001b[39m0.3366   \u001b[39m | \u001b[39m0.2497   \u001b[39m | \u001b[39m0.399    \u001b[39m | \u001b[39m0.1549   \u001b[39m | \u001b[39m0.1509   \u001b[39m | \u001b[39m0.3572   \u001b[39m | \u001b[39m0.3556   \u001b[39m | \u001b[39m0.2783   \u001b[39m |\n",
      "| \u001b[39m64       \u001b[39m | \u001b[39m0.04598  \u001b[39m | \u001b[39m0.2773   \u001b[39m | \u001b[39m0.2698   \u001b[39m | \u001b[39m0.4609   \u001b[39m | \u001b[39m0.3889   \u001b[39m | \u001b[39m0.3134   \u001b[39m | \u001b[39m0.2989   \u001b[39m | \u001b[39m0.1203   \u001b[39m | \u001b[39m0.4048   \u001b[39m | \u001b[39m0.4595   \u001b[39m | \u001b[39m0.3605   \u001b[39m | \u001b[39m0.1273   \u001b[39m | \u001b[39m0.1037   \u001b[39m |\n",
      "| \u001b[39m65       \u001b[39m | \u001b[39m0.06231  \u001b[39m | \u001b[39m0.2538   \u001b[39m | \u001b[39m0.3414   \u001b[39m | \u001b[39m0.3224   \u001b[39m | \u001b[39m0.1978   \u001b[39m | \u001b[39m0.446    \u001b[39m | \u001b[39m0.1252   \u001b[39m | \u001b[39m0.3336   \u001b[39m | \u001b[39m0.437    \u001b[39m | \u001b[39m0.2633   \u001b[39m | \u001b[39m0.2136   \u001b[39m | \u001b[39m0.2632   \u001b[39m | \u001b[39m0.4159   \u001b[39m |\n",
      "| \u001b[39m66       \u001b[39m | \u001b[39m0.06518  \u001b[39m | \u001b[39m0.3696   \u001b[39m | \u001b[39m0.3904   \u001b[39m | \u001b[39m0.2668   \u001b[39m | \u001b[39m0.3712   \u001b[39m | \u001b[39m0.1472   \u001b[39m | \u001b[39m0.4482   \u001b[39m | \u001b[39m0.2149   \u001b[39m | \u001b[39m0.1887   \u001b[39m | \u001b[39m0.2231   \u001b[39m | \u001b[39m0.1855   \u001b[39m | \u001b[39m0.4061   \u001b[39m | \u001b[39m0.4306   \u001b[39m |\n",
      "| \u001b[35m67       \u001b[39m | \u001b[35m0.1259   \u001b[39m | \u001b[35m0.494    \u001b[39m | \u001b[35m0.1967   \u001b[39m | \u001b[35m0.3889   \u001b[39m | \u001b[35m0.446    \u001b[39m | \u001b[35m0.3571   \u001b[39m | \u001b[35m0.1009   \u001b[39m | \u001b[35m0.364    \u001b[39m | \u001b[35m0.3278   \u001b[39m | \u001b[35m0.2478   \u001b[39m | \u001b[35m0.1692   \u001b[39m | \u001b[35m0.2152   \u001b[39m | \u001b[35m0.1353   \u001b[39m |\n",
      "| \u001b[39m68       \u001b[39m | \u001b[39m0.0981   \u001b[39m | \u001b[39m0.494    \u001b[39m | \u001b[39m0.1968   \u001b[39m | \u001b[39m0.3889   \u001b[39m | \u001b[39m0.446    \u001b[39m | \u001b[39m0.3571   \u001b[39m | \u001b[39m0.1009   \u001b[39m | \u001b[39m0.364    \u001b[39m | \u001b[39m0.3278   \u001b[39m | \u001b[39m0.2478   \u001b[39m | \u001b[39m0.1693   \u001b[39m | \u001b[39m0.2153   \u001b[39m | \u001b[39m0.1354   \u001b[39m |\n",
      "| \u001b[39m69       \u001b[39m | \u001b[39m0.01548  \u001b[39m | \u001b[39m0.4938   \u001b[39m | \u001b[39m0.4383   \u001b[39m | \u001b[39m0.2063   \u001b[39m | \u001b[39m0.3458   \u001b[39m | \u001b[39m0.1374   \u001b[39m | \u001b[39m0.452    \u001b[39m | \u001b[39m0.1071   \u001b[39m | \u001b[39m0.3359   \u001b[39m | \u001b[39m0.4654   \u001b[39m | \u001b[39m0.4184   \u001b[39m | \u001b[39m0.4527   \u001b[39m | \u001b[39m0.1373   \u001b[39m |\n",
      "| \u001b[39m70       \u001b[39m | \u001b[39m0.03687  \u001b[39m | \u001b[39m0.2755   \u001b[39m | \u001b[39m0.4554   \u001b[39m | \u001b[39m0.2482   \u001b[39m | \u001b[39m0.1654   \u001b[39m | \u001b[39m0.1164   \u001b[39m | \u001b[39m0.1683   \u001b[39m | \u001b[39m0.2481   \u001b[39m | \u001b[39m0.3282   \u001b[39m | \u001b[39m0.3315   \u001b[39m | \u001b[39m0.2325   \u001b[39m | \u001b[39m0.306    \u001b[39m | \u001b[39m0.1978   \u001b[39m |\n",
      "| \u001b[39m71       \u001b[39m | \u001b[39m0.07483  \u001b[39m | \u001b[39m0.1062   \u001b[39m | \u001b[39m0.3849   \u001b[39m | \u001b[39m0.4629   \u001b[39m | \u001b[39m0.283    \u001b[39m | \u001b[39m0.3842   \u001b[39m | \u001b[39m0.3626   \u001b[39m | \u001b[39m0.4613   \u001b[39m | \u001b[39m0.3148   \u001b[39m | \u001b[39m0.1219   \u001b[39m | \u001b[39m0.1428   \u001b[39m | \u001b[39m0.2558   \u001b[39m | \u001b[39m0.3289   \u001b[39m |\n",
      "| \u001b[39m72       \u001b[39m | \u001b[39m0.1024   \u001b[39m | \u001b[39m0.3826   \u001b[39m | \u001b[39m0.2529   \u001b[39m | \u001b[39m0.1726   \u001b[39m | \u001b[39m0.2357   \u001b[39m | \u001b[39m0.2376   \u001b[39m | \u001b[39m0.2303   \u001b[39m | \u001b[39m0.1674   \u001b[39m | \u001b[39m0.265    \u001b[39m | \u001b[39m0.4436   \u001b[39m | \u001b[39m0.4716   \u001b[39m | \u001b[39m0.428    \u001b[39m | \u001b[39m0.4791   \u001b[39m |\n",
      "| \u001b[39m73       \u001b[39m | \u001b[39m0.02619  \u001b[39m | \u001b[39m0.1386   \u001b[39m | \u001b[39m0.1285   \u001b[39m | \u001b[39m0.2072   \u001b[39m | \u001b[39m0.3259   \u001b[39m | \u001b[39m0.3731   \u001b[39m | \u001b[39m0.141    \u001b[39m | \u001b[39m0.3067   \u001b[39m | \u001b[39m0.2455   \u001b[39m | \u001b[39m0.2925   \u001b[39m | \u001b[39m0.3511   \u001b[39m | \u001b[39m0.4686   \u001b[39m | \u001b[39m0.3612   \u001b[39m |\n",
      "| \u001b[39m74       \u001b[39m | \u001b[39m0.02735  \u001b[39m | \u001b[39m0.3393   \u001b[39m | \u001b[39m0.4098   \u001b[39m | \u001b[39m0.1573   \u001b[39m | \u001b[39m0.2089   \u001b[39m | \u001b[39m0.2208   \u001b[39m | \u001b[39m0.3128   \u001b[39m | \u001b[39m0.1752   \u001b[39m | \u001b[39m0.3581   \u001b[39m | \u001b[39m0.1084   \u001b[39m | \u001b[39m0.1036   \u001b[39m | \u001b[39m0.4862   \u001b[39m | \u001b[39m0.3585   \u001b[39m |\n",
      "| \u001b[39m75       \u001b[39m | \u001b[39m0.02019  \u001b[39m | \u001b[39m0.2494   \u001b[39m | \u001b[39m0.2873   \u001b[39m | \u001b[39m0.1982   \u001b[39m | \u001b[39m0.3268   \u001b[39m | \u001b[39m0.3436   \u001b[39m | \u001b[39m0.3231   \u001b[39m | \u001b[39m0.3248   \u001b[39m | \u001b[39m0.324    \u001b[39m | \u001b[39m0.4028   \u001b[39m | \u001b[39m0.1672   \u001b[39m | \u001b[39m0.3215   \u001b[39m | \u001b[39m0.2424   \u001b[39m |\n",
      "| \u001b[39m76       \u001b[39m | \u001b[39m0.0947   \u001b[39m | \u001b[39m0.2959   \u001b[39m | \u001b[39m0.4608   \u001b[39m | \u001b[39m0.2525   \u001b[39m | \u001b[39m0.4316   \u001b[39m | \u001b[39m0.1671   \u001b[39m | \u001b[39m0.292    \u001b[39m | \u001b[39m0.1703   \u001b[39m | \u001b[39m0.3341   \u001b[39m | \u001b[39m0.3806   \u001b[39m | \u001b[39m0.1043   \u001b[39m | \u001b[39m0.387    \u001b[39m | \u001b[39m0.4181   \u001b[39m |\n",
      "| \u001b[39m77       \u001b[39m | \u001b[39m0.0643   \u001b[39m | \u001b[39m0.3413   \u001b[39m | \u001b[39m0.3415   \u001b[39m | \u001b[39m0.3175   \u001b[39m | \u001b[39m0.2052   \u001b[39m | \u001b[39m0.352    \u001b[39m | \u001b[39m0.4557   \u001b[39m | \u001b[39m0.3612   \u001b[39m | \u001b[39m0.1155   \u001b[39m | \u001b[39m0.4652   \u001b[39m | \u001b[39m0.1528   \u001b[39m | \u001b[39m0.3598   \u001b[39m | \u001b[39m0.4515   \u001b[39m |\n",
      "| \u001b[39m78       \u001b[39m | \u001b[39m0.02569  \u001b[39m | \u001b[39m0.4329   \u001b[39m | \u001b[39m0.2962   \u001b[39m | \u001b[39m0.2004   \u001b[39m | \u001b[39m0.2834   \u001b[39m | \u001b[39m0.2082   \u001b[39m | \u001b[39m0.253    \u001b[39m | \u001b[39m0.1862   \u001b[39m | \u001b[39m0.3342   \u001b[39m | \u001b[39m0.401    \u001b[39m | \u001b[39m0.1362   \u001b[39m | \u001b[39m0.1488   \u001b[39m | \u001b[39m0.3836   \u001b[39m |\n",
      "| \u001b[39m79       \u001b[39m | \u001b[39m0.07901  \u001b[39m | \u001b[39m0.3865   \u001b[39m | \u001b[39m0.2864   \u001b[39m | \u001b[39m0.2686   \u001b[39m | \u001b[39m0.1711   \u001b[39m | \u001b[39m0.2879   \u001b[39m | \u001b[39m0.1235   \u001b[39m | \u001b[39m0.4353   \u001b[39m | \u001b[39m0.2563   \u001b[39m | \u001b[39m0.2752   \u001b[39m | \u001b[39m0.4003   \u001b[39m | \u001b[39m0.4942   \u001b[39m | \u001b[39m0.3944   \u001b[39m |\n",
      "| \u001b[39m80       \u001b[39m | \u001b[39m0.04906  \u001b[39m | \u001b[39m0.2314   \u001b[39m | \u001b[39m0.4396   \u001b[39m | \u001b[39m0.3844   \u001b[39m | \u001b[39m0.3221   \u001b[39m | \u001b[39m0.3992   \u001b[39m | \u001b[39m0.166    \u001b[39m | \u001b[39m0.2575   \u001b[39m | \u001b[39m0.3408   \u001b[39m | \u001b[39m0.3552   \u001b[39m | \u001b[39m0.1204   \u001b[39m | \u001b[39m0.4035   \u001b[39m | \u001b[39m0.1554   \u001b[39m |\n",
      "| \u001b[39m81       \u001b[39m | \u001b[39m0.04683  \u001b[39m | \u001b[39m0.4365   \u001b[39m | \u001b[39m0.1375   \u001b[39m | \u001b[39m0.483    \u001b[39m | \u001b[39m0.3307   \u001b[39m | \u001b[39m0.421    \u001b[39m | \u001b[39m0.4131   \u001b[39m | \u001b[39m0.2309   \u001b[39m | \u001b[39m0.4303   \u001b[39m | \u001b[39m0.1836   \u001b[39m | \u001b[39m0.2886   \u001b[39m | \u001b[39m0.1979   \u001b[39m | \u001b[39m0.4037   \u001b[39m |\n",
      "| \u001b[39m82       \u001b[39m | \u001b[39m0.08422  \u001b[39m | \u001b[39m0.494    \u001b[39m | \u001b[39m0.1967   \u001b[39m | \u001b[39m0.3889   \u001b[39m | \u001b[39m0.446    \u001b[39m | \u001b[39m0.3571   \u001b[39m | \u001b[39m0.1009   \u001b[39m | \u001b[39m0.364    \u001b[39m | \u001b[39m0.3278   \u001b[39m | \u001b[39m0.2478   \u001b[39m | \u001b[39m0.1692   \u001b[39m | \u001b[39m0.2152   \u001b[39m | \u001b[39m0.1353   \u001b[39m |\n",
      "| \u001b[39m83       \u001b[39m | \u001b[39m0.111    \u001b[39m | \u001b[39m0.2558   \u001b[39m | \u001b[39m0.1733   \u001b[39m | \u001b[39m0.3289   \u001b[39m | \u001b[39m0.4962   \u001b[39m | \u001b[39m0.1927   \u001b[39m | \u001b[39m0.1706   \u001b[39m | \u001b[39m0.2359   \u001b[39m | \u001b[39m0.2583   \u001b[39m | \u001b[39m0.4384   \u001b[39m | \u001b[39m0.3736   \u001b[39m | \u001b[39m0.4703   \u001b[39m | \u001b[39m0.3244   \u001b[39m |\n",
      "| \u001b[39m84       \u001b[39m | \u001b[39m0.0888   \u001b[39m | \u001b[39m0.2914   \u001b[39m | \u001b[39m0.3064   \u001b[39m | \u001b[39m0.3389   \u001b[39m | \u001b[39m0.1691   \u001b[39m | \u001b[39m0.3515   \u001b[39m | \u001b[39m0.2257   \u001b[39m | \u001b[39m0.1724   \u001b[39m | \u001b[39m0.428    \u001b[39m | \u001b[39m0.2236   \u001b[39m | \u001b[39m0.4894   \u001b[39m | \u001b[39m0.2492   \u001b[39m | \u001b[39m0.4428   \u001b[39m |\n",
      "| \u001b[39m85       \u001b[39m | \u001b[39m0.02586  \u001b[39m | \u001b[39m0.3264   \u001b[39m | \u001b[39m0.2667   \u001b[39m | \u001b[39m0.3558   \u001b[39m | \u001b[39m0.261    \u001b[39m | \u001b[39m0.4647   \u001b[39m | \u001b[39m0.3759   \u001b[39m | \u001b[39m0.1786   \u001b[39m | \u001b[39m0.2984   \u001b[39m | \u001b[39m0.2142   \u001b[39m | \u001b[39m0.227    \u001b[39m | \u001b[39m0.2803   \u001b[39m | \u001b[39m0.4592   \u001b[39m |\n",
      "| \u001b[39m86       \u001b[39m | \u001b[39m0.02836  \u001b[39m | \u001b[39m0.1814   \u001b[39m | \u001b[39m0.4134   \u001b[39m | \u001b[39m0.1931   \u001b[39m | \u001b[39m0.3094   \u001b[39m | \u001b[39m0.2525   \u001b[39m | \u001b[39m0.1897   \u001b[39m | \u001b[39m0.4677   \u001b[39m | \u001b[39m0.3801   \u001b[39m | \u001b[39m0.4558   \u001b[39m | \u001b[39m0.1657   \u001b[39m | \u001b[39m0.2393   \u001b[39m | \u001b[39m0.2036   \u001b[39m |\n",
      "| \u001b[39m87       \u001b[39m | \u001b[39m0.04055  \u001b[39m | \u001b[39m0.2158   \u001b[39m | \u001b[39m0.1638   \u001b[39m | \u001b[39m0.2809   \u001b[39m | \u001b[39m0.2785   \u001b[39m | \u001b[39m0.153    \u001b[39m | \u001b[39m0.4427   \u001b[39m | \u001b[39m0.4859   \u001b[39m | \u001b[39m0.4367   \u001b[39m | \u001b[39m0.3215   \u001b[39m | \u001b[39m0.2851   \u001b[39m | \u001b[39m0.3335   \u001b[39m | \u001b[39m0.2758   \u001b[39m |\n",
      "| \u001b[39m88       \u001b[39m | \u001b[39m0.03477  \u001b[39m | \u001b[39m0.4985   \u001b[39m | \u001b[39m0.3517   \u001b[39m | \u001b[39m0.2027   \u001b[39m | \u001b[39m0.48     \u001b[39m | \u001b[39m0.4581   \u001b[39m | \u001b[39m0.259    \u001b[39m | \u001b[39m0.2941   \u001b[39m | \u001b[39m0.2136   \u001b[39m | \u001b[39m0.2964   \u001b[39m | \u001b[39m0.2274   \u001b[39m | \u001b[39m0.3049   \u001b[39m | \u001b[39m0.2663   \u001b[39m |\n",
      "| \u001b[39m89       \u001b[39m | \u001b[39m0.009933 \u001b[39m | \u001b[39m0.3528   \u001b[39m | \u001b[39m0.1016   \u001b[39m | \u001b[39m0.449    \u001b[39m | \u001b[39m0.2059   \u001b[39m | \u001b[39m0.4385   \u001b[39m | \u001b[39m0.3297   \u001b[39m | \u001b[39m0.1002   \u001b[39m | \u001b[39m0.2677   \u001b[39m | \u001b[39m0.1314   \u001b[39m | \u001b[39m0.3274   \u001b[39m | \u001b[39m0.1044   \u001b[39m | \u001b[39m0.3497   \u001b[39m |\n",
      "| \u001b[39m90       \u001b[39m | \u001b[39m0.02519  \u001b[39m | \u001b[39m0.2181   \u001b[39m | \u001b[39m0.2019   \u001b[39m | \u001b[39m0.183    \u001b[39m | \u001b[39m0.4567   \u001b[39m | \u001b[39m0.4326   \u001b[39m | \u001b[39m0.3022   \u001b[39m | \u001b[39m0.1611   \u001b[39m | \u001b[39m0.1407   \u001b[39m | \u001b[39m0.4325   \u001b[39m | \u001b[39m0.2622   \u001b[39m | \u001b[39m0.1401   \u001b[39m | \u001b[39m0.4487   \u001b[39m |\n",
      "| \u001b[39m91       \u001b[39m | \u001b[39m0.0531   \u001b[39m | \u001b[39m0.1245   \u001b[39m | \u001b[39m0.462    \u001b[39m | \u001b[39m0.2252   \u001b[39m | \u001b[39m0.4011   \u001b[39m | \u001b[39m0.2131   \u001b[39m | \u001b[39m0.2353   \u001b[39m | \u001b[39m0.3433   \u001b[39m | \u001b[39m0.3332   \u001b[39m | \u001b[39m0.3283   \u001b[39m | \u001b[39m0.2859   \u001b[39m | \u001b[39m0.4454   \u001b[39m | \u001b[39m0.2932   \u001b[39m |\n",
      "| \u001b[39m92       \u001b[39m | \u001b[39m0.003634 \u001b[39m | \u001b[39m0.1561   \u001b[39m | \u001b[39m0.4972   \u001b[39m | \u001b[39m0.3518   \u001b[39m | \u001b[39m0.3147   \u001b[39m | \u001b[39m0.1897   \u001b[39m | \u001b[39m0.4224   \u001b[39m | \u001b[39m0.3132   \u001b[39m | \u001b[39m0.163    \u001b[39m | \u001b[39m0.231    \u001b[39m | \u001b[39m0.2742   \u001b[39m | \u001b[39m0.1062   \u001b[39m | \u001b[39m0.4912   \u001b[39m |\n",
      "| \u001b[39m93       \u001b[39m | \u001b[39m0.05009  \u001b[39m | \u001b[39m0.1051   \u001b[39m | \u001b[39m0.2331   \u001b[39m | \u001b[39m0.4152   \u001b[39m | \u001b[39m0.132    \u001b[39m | \u001b[39m0.148    \u001b[39m | \u001b[39m0.1118   \u001b[39m | \u001b[39m0.4854   \u001b[39m | \u001b[39m0.4134   \u001b[39m | \u001b[39m0.361    \u001b[39m | \u001b[39m0.4214   \u001b[39m | \u001b[39m0.1115   \u001b[39m | \u001b[39m0.4424   \u001b[39m |\n",
      "| \u001b[39m94       \u001b[39m | \u001b[39m0.08422  \u001b[39m | \u001b[39m0.494    \u001b[39m | \u001b[39m0.1967   \u001b[39m | \u001b[39m0.3889   \u001b[39m | \u001b[39m0.446    \u001b[39m | \u001b[39m0.3571   \u001b[39m | \u001b[39m0.1009   \u001b[39m | \u001b[39m0.3639   \u001b[39m | \u001b[39m0.3278   \u001b[39m | \u001b[39m0.2478   \u001b[39m | \u001b[39m0.1692   \u001b[39m | \u001b[39m0.2152   \u001b[39m | \u001b[39m0.1353   \u001b[39m |\n",
      "| \u001b[39m95       \u001b[39m | \u001b[39m0.1232   \u001b[39m | \u001b[39m0.1349   \u001b[39m | \u001b[39m0.1035   \u001b[39m | \u001b[39m0.1242   \u001b[39m | \u001b[39m0.1814   \u001b[39m | \u001b[39m0.3403   \u001b[39m | \u001b[39m0.1357   \u001b[39m | \u001b[39m0.404    \u001b[39m | \u001b[39m0.4655   \u001b[39m | \u001b[39m0.2952   \u001b[39m | \u001b[39m0.2874   \u001b[39m | \u001b[39m0.4116   \u001b[39m | \u001b[39m0.4084   \u001b[39m |\n",
      "| \u001b[39m96       \u001b[39m | \u001b[39m0.06094  \u001b[39m | \u001b[39m0.4058   \u001b[39m | \u001b[39m0.3895   \u001b[39m | \u001b[39m0.2472   \u001b[39m | \u001b[39m0.3112   \u001b[39m | \u001b[39m0.1523   \u001b[39m | \u001b[39m0.21     \u001b[39m | \u001b[39m0.4391   \u001b[39m | \u001b[39m0.3397   \u001b[39m | \u001b[39m0.2652   \u001b[39m | \u001b[39m0.3927   \u001b[39m | \u001b[39m0.3438   \u001b[39m | \u001b[39m0.1786   \u001b[39m |\n",
      "| \u001b[39m97       \u001b[39m | \u001b[39m0.06637  \u001b[39m | \u001b[39m0.2966   \u001b[39m | \u001b[39m0.4186   \u001b[39m | \u001b[39m0.2569   \u001b[39m | \u001b[39m0.2509   \u001b[39m | \u001b[39m0.1193   \u001b[39m | \u001b[39m0.1523   \u001b[39m | \u001b[39m0.4725   \u001b[39m | \u001b[39m0.1161   \u001b[39m | \u001b[39m0.1071   \u001b[39m | \u001b[39m0.237    \u001b[39m | \u001b[39m0.2536   \u001b[39m | \u001b[39m0.2155   \u001b[39m |\n",
      "| \u001b[39m98       \u001b[39m | \u001b[39m0.05101  \u001b[39m | \u001b[39m0.175    \u001b[39m | \u001b[39m0.3941   \u001b[39m | \u001b[39m0.3888   \u001b[39m | \u001b[39m0.4747   \u001b[39m | \u001b[39m0.2203   \u001b[39m | \u001b[39m0.1479   \u001b[39m | \u001b[39m0.4617   \u001b[39m | \u001b[39m0.3311   \u001b[39m | \u001b[39m0.1847   \u001b[39m | \u001b[39m0.4577   \u001b[39m | \u001b[39m0.137    \u001b[39m | \u001b[39m0.3033   \u001b[39m |\n",
      "| \u001b[39m99       \u001b[39m | \u001b[39m0.05072  \u001b[39m | \u001b[39m0.425    \u001b[39m | \u001b[39m0.4021   \u001b[39m | \u001b[39m0.4133   \u001b[39m | \u001b[39m0.4204   \u001b[39m | \u001b[39m0.1959   \u001b[39m | \u001b[39m0.4047   \u001b[39m | \u001b[39m0.3928   \u001b[39m | \u001b[39m0.2733   \u001b[39m | \u001b[39m0.2712   \u001b[39m | \u001b[39m0.2507   \u001b[39m | \u001b[39m0.4972   \u001b[39m | \u001b[39m0.4461   \u001b[39m |\n",
      "| \u001b[39m100      \u001b[39m | \u001b[39m0.04332  \u001b[39m | \u001b[39m0.4057   \u001b[39m | \u001b[39m0.4954   \u001b[39m | \u001b[39m0.4618   \u001b[39m | \u001b[39m0.1499   \u001b[39m | \u001b[39m0.3467   \u001b[39m | \u001b[39m0.4027   \u001b[39m | \u001b[39m0.3412   \u001b[39m | \u001b[39m0.3004   \u001b[39m | \u001b[39m0.4683   \u001b[39m | \u001b[39m0.198    \u001b[39m | \u001b[39m0.3787   \u001b[39m | \u001b[39m0.1437   \u001b[39m |\n",
      "| \u001b[39m101      \u001b[39m | \u001b[39m0.07083  \u001b[39m | \u001b[39m0.498    \u001b[39m | \u001b[39m0.4052   \u001b[39m | \u001b[39m0.2954   \u001b[39m | \u001b[39m0.4414   \u001b[39m | \u001b[39m0.4402   \u001b[39m | \u001b[39m0.4023   \u001b[39m | \u001b[39m0.168    \u001b[39m | \u001b[39m0.2908   \u001b[39m | \u001b[39m0.431    \u001b[39m | \u001b[39m0.1253   \u001b[39m | \u001b[39m0.1464   \u001b[39m | \u001b[39m0.1749   \u001b[39m |\n",
      "| \u001b[39m102      \u001b[39m | \u001b[39m0.03494  \u001b[39m | \u001b[39m0.3555   \u001b[39m | \u001b[39m0.1158   \u001b[39m | \u001b[39m0.2302   \u001b[39m | \u001b[39m0.3307   \u001b[39m | \u001b[39m0.4601   \u001b[39m | \u001b[39m0.1723   \u001b[39m | \u001b[39m0.3388   \u001b[39m | \u001b[39m0.1427   \u001b[39m | \u001b[39m0.1365   \u001b[39m | \u001b[39m0.2462   \u001b[39m | \u001b[39m0.425    \u001b[39m | \u001b[39m0.414    \u001b[39m |\n",
      "| \u001b[39m103      \u001b[39m | \u001b[39m0.03815  \u001b[39m | \u001b[39m0.4523   \u001b[39m | \u001b[39m0.4162   \u001b[39m | \u001b[39m0.4014   \u001b[39m | \u001b[39m0.4492   \u001b[39m | \u001b[39m0.4385   \u001b[39m | \u001b[39m0.4394   \u001b[39m | \u001b[39m0.4012   \u001b[39m | \u001b[39m0.1773   \u001b[39m | \u001b[39m0.2134   \u001b[39m | \u001b[39m0.4353   \u001b[39m | \u001b[39m0.4672   \u001b[39m | \u001b[39m0.3851   \u001b[39m |\n",
      "| \u001b[39m104      \u001b[39m | \u001b[39m0.112    \u001b[39m | \u001b[39m0.494    \u001b[39m | \u001b[39m0.1967   \u001b[39m | \u001b[39m0.3889   \u001b[39m | \u001b[39m0.446    \u001b[39m | \u001b[39m0.3571   \u001b[39m | \u001b[39m0.1009   \u001b[39m | \u001b[39m0.3639   \u001b[39m | \u001b[39m0.3278   \u001b[39m | \u001b[39m0.2478   \u001b[39m | \u001b[39m0.1692   \u001b[39m | \u001b[39m0.2152   \u001b[39m | \u001b[39m0.1353   \u001b[39m |\n",
      "| \u001b[39m105      \u001b[39m | \u001b[39m0.05644  \u001b[39m | \u001b[39m0.4626   \u001b[39m | \u001b[39m0.2283   \u001b[39m | \u001b[39m0.1334   \u001b[39m | \u001b[39m0.1389   \u001b[39m | \u001b[39m0.1701   \u001b[39m | \u001b[39m0.3995   \u001b[39m | \u001b[39m0.4907   \u001b[39m | \u001b[39m0.4204   \u001b[39m | \u001b[39m0.3625   \u001b[39m | \u001b[39m0.4852   \u001b[39m | \u001b[39m0.2033   \u001b[39m | \u001b[39m0.3577   \u001b[39m |\n",
      "| \u001b[39m106      \u001b[39m | \u001b[39m0.0645   \u001b[39m | \u001b[39m0.3866   \u001b[39m | \u001b[39m0.2331   \u001b[39m | \u001b[39m0.2768   \u001b[39m | \u001b[39m0.4962   \u001b[39m | \u001b[39m0.1013   \u001b[39m | \u001b[39m0.4069   \u001b[39m | \u001b[39m0.282    \u001b[39m | \u001b[39m0.397    \u001b[39m | \u001b[39m0.225    \u001b[39m | \u001b[39m0.3405   \u001b[39m | \u001b[39m0.46     \u001b[39m | \u001b[39m0.2554   \u001b[39m |\n",
      "| \u001b[39m107      \u001b[39m | \u001b[39m0.02977  \u001b[39m | \u001b[39m0.1929   \u001b[39m | \u001b[39m0.4519   \u001b[39m | \u001b[39m0.1092   \u001b[39m | \u001b[39m0.1293   \u001b[39m | \u001b[39m0.3738   \u001b[39m | \u001b[39m0.3624   \u001b[39m | \u001b[39m0.2831   \u001b[39m | \u001b[39m0.382    \u001b[39m | \u001b[39m0.3725   \u001b[39m | \u001b[39m0.4967   \u001b[39m | \u001b[39m0.1232   \u001b[39m | \u001b[39m0.4023   \u001b[39m |\n",
      "| \u001b[39m108      \u001b[39m | \u001b[39m0.05315  \u001b[39m | \u001b[39m0.2922   \u001b[39m | \u001b[39m0.3027   \u001b[39m | \u001b[39m0.4036   \u001b[39m | \u001b[39m0.2197   \u001b[39m | \u001b[39m0.2291   \u001b[39m | \u001b[39m0.4708   \u001b[39m | \u001b[39m0.269    \u001b[39m | \u001b[39m0.2084   \u001b[39m | \u001b[39m0.3609   \u001b[39m | \u001b[39m0.2322   \u001b[39m | \u001b[39m0.4815   \u001b[39m | \u001b[39m0.2054   \u001b[39m |\n",
      "| \u001b[39m109      \u001b[39m | \u001b[39m0.03335  \u001b[39m | \u001b[39m0.1725   \u001b[39m | \u001b[39m0.2611   \u001b[39m | \u001b[39m0.4727   \u001b[39m | \u001b[39m0.2615   \u001b[39m | \u001b[39m0.3499   \u001b[39m | \u001b[39m0.4028   \u001b[39m | \u001b[39m0.1396   \u001b[39m | \u001b[39m0.1003   \u001b[39m | \u001b[39m0.2806   \u001b[39m | \u001b[39m0.4148   \u001b[39m | \u001b[39m0.4296   \u001b[39m | \u001b[39m0.3694   \u001b[39m |\n",
      "| \u001b[39m110      \u001b[39m | \u001b[39m0.05157  \u001b[39m | \u001b[39m0.1338   \u001b[39m | \u001b[39m0.3766   \u001b[39m | \u001b[39m0.3953   \u001b[39m | \u001b[39m0.4536   \u001b[39m | \u001b[39m0.1805   \u001b[39m | \u001b[39m0.1156   \u001b[39m | \u001b[39m0.3572   \u001b[39m | \u001b[39m0.4934   \u001b[39m | \u001b[39m0.2076   \u001b[39m | \u001b[39m0.3254   \u001b[39m | \u001b[39m0.4018   \u001b[39m | \u001b[39m0.2477   \u001b[39m |\n",
      "=========================================================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "best_configs = optimize_peer_models(base_model, num_peers=3, num_layers=4, base_heads=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gpt_sparsity(model):\n",
    "    total_params = 0\n",
    "    zero_params = 0\n",
    "\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, (blm.gpt_2.attention.CausalSelfAttention, blm.gpt_2.elements.MLP)):\n",
    "            for param_name, param in module.named_parameters():\n",
    "                if 'weight' in param_name:\n",
    "                    layer_total = param.nelement()\n",
    "                    layer_zero = torch.sum(param == 0).item()\n",
    "                    layer_sparsity = 100.0 * layer_zero / layer_total\n",
    "                    print(f\"{name}.{param_name}: {layer_sparsity:.2f}% sparsity\")\n",
    "                    total_params += layer_total\n",
    "                    zero_params += layer_zero\n",
    "    overall_sparsity = 100.0 * zero_params / total_params\n",
    "    print(f\"Overall model sparsity: {overall_sparsity:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pruned model 1\n",
      "transformer.h.0.attn.c_attn.weight: 33.33% sparsity\n",
      "transformer.h.0.attn.c_proj.weight: 100.00% sparsity\n",
      "transformer.h.0.mlp.c_fc.weight: 49.40% sparsity\n",
      "transformer.h.0.mlp.c_proj.weight: 49.40% sparsity\n",
      "transformer.h.1.attn.c_attn.weight: 33.33% sparsity\n",
      "transformer.h.1.attn.c_proj.weight: 100.00% sparsity\n",
      "transformer.h.1.mlp.c_fc.weight: 19.67% sparsity\n",
      "transformer.h.1.mlp.c_proj.weight: 19.67% sparsity\n",
      "transformer.h.2.attn.c_attn.weight: 16.67% sparsity\n",
      "transformer.h.2.attn.c_proj.weight: 50.00% sparsity\n",
      "transformer.h.2.mlp.c_fc.weight: 38.89% sparsity\n",
      "transformer.h.2.mlp.c_proj.weight: 38.89% sparsity\n",
      "transformer.h.3.attn.c_attn.weight: 8.33% sparsity\n",
      "transformer.h.3.attn.c_proj.weight: 25.00% sparsity\n",
      "transformer.h.3.mlp.c_fc.weight: 44.60% sparsity\n",
      "transformer.h.3.mlp.c_proj.weight: 44.60% sparsity\n",
      "Overall model sparsity: 36.88%\n",
      "pruned model 2\n",
      "transformer.h.0.attn.c_attn.weight: 33.33% sparsity\n",
      "transformer.h.0.attn.c_proj.weight: 100.00% sparsity\n",
      "transformer.h.0.mlp.c_fc.weight: 35.71% sparsity\n",
      "transformer.h.0.mlp.c_proj.weight: 35.71% sparsity\n",
      "transformer.h.1.attn.c_attn.weight: 16.67% sparsity\n",
      "transformer.h.1.attn.c_proj.weight: 50.00% sparsity\n",
      "transformer.h.1.mlp.c_fc.weight: 10.09% sparsity\n",
      "transformer.h.1.mlp.c_proj.weight: 10.09% sparsity\n",
      "transformer.h.2.attn.c_attn.weight: 8.33% sparsity\n",
      "transformer.h.2.attn.c_proj.weight: 25.00% sparsity\n",
      "transformer.h.2.mlp.c_fc.weight: 36.40% sparsity\n",
      "transformer.h.2.mlp.c_proj.weight: 36.40% sparsity\n",
      "transformer.h.3.attn.c_attn.weight: 8.33% sparsity\n",
      "transformer.h.3.attn.c_proj.weight: 25.00% sparsity\n",
      "transformer.h.3.mlp.c_fc.weight: 32.78% sparsity\n",
      "transformer.h.3.mlp.c_proj.weight: 32.78% sparsity\n",
      "Overall model sparsity: 27.50%\n",
      "pruned model 3\n",
      "transformer.h.0.attn.c_attn.weight: 33.33% sparsity\n",
      "transformer.h.0.attn.c_proj.weight: 100.00% sparsity\n",
      "transformer.h.0.mlp.c_fc.weight: 24.78% sparsity\n",
      "transformer.h.0.mlp.c_proj.weight: 24.78% sparsity\n",
      "transformer.h.1.attn.c_attn.weight: 16.67% sparsity\n",
      "transformer.h.1.attn.c_proj.weight: 50.00% sparsity\n",
      "transformer.h.1.mlp.c_fc.weight: 16.92% sparsity\n",
      "transformer.h.1.mlp.c_proj.weight: 16.92% sparsity\n",
      "transformer.h.2.attn.c_attn.weight: 16.67% sparsity\n",
      "transformer.h.2.attn.c_proj.weight: 50.00% sparsity\n",
      "transformer.h.2.mlp.c_fc.weight: 21.52% sparsity\n",
      "transformer.h.2.mlp.c_proj.weight: 21.52% sparsity\n",
      "transformer.h.3.attn.c_attn.weight: 16.67% sparsity\n",
      "transformer.h.3.attn.c_proj.weight: 50.00% sparsity\n",
      "transformer.h.3.mlp.c_fc.weight: 13.53% sparsity\n",
      "transformer.h.3.mlp.c_proj.weight: 13.53% sparsity\n",
      "Overall model sparsity: 23.21%\n"
     ]
    }
   ],
   "source": [
    "for i, config in enumerate(best_configs):\n",
    "    print(f\"pruned model {i+1}\")\n",
    "    pruned_model = prune_gpt_model(base_model,config)\n",
    "    print_gpt_sparsity(pruned_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baby-lm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
